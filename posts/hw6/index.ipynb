{
 "cells": [
  {
   "cell_type": "raw",
   "id": "da77bca7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"HW6\"\n",
    "author: \"Jessica Xiao\"\n",
    "date: \"2024-3-11\"\n",
    "categories: [week 10, homework]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fa8cd",
   "metadata": {},
   "source": [
    "# HW6: Fake News Classification\n",
    "In this homework, we are going to create a fake news classifier with Keras. \n",
    "\n",
    "First, we will import the necessary libraries.\n",
    "```python\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import keras\n",
    "from keras import layers, losses\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# for embedding viz\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "## Acquire training data\n",
    "The data that we will use to train the model can be found at the link below. We will read the data into a pandas dataframe.\n",
    "```python\n",
    "train_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n",
    "\n",
    "train_data = pd.read_csv(train_url)\n",
    "train_data\n",
    "```\n",
    "![output](hw6-traindata.png)\n",
    "We see that this dataset contains information about 22449 articles. Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.\n",
    "\n",
    "## Create a dataset\n",
    "Next, let's write the function make_dataset to convert the data to a tensorflow dataset.\n",
    "```python\n",
    "batch_size = 100\n",
    "def make_dataset(df):\n",
    "    # Load stopwords from NLTK\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # remove stopwords\n",
    "    df['title'] = df['title'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    df['text'] = df['text'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    \n",
    "    # Construct a tf.data.Dataset\n",
    "    # Note: Ensure your DataFrame has columns named 'title', 'text', and 'fake' for this to work directly\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"title\": df['title'].values,  # Pass series.values to get numpy representation\n",
    "                \"text\": df['text'].values\n",
    "            },\n",
    "            df['fake'].values\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = make_dataset(train_data)\n",
    "```\n",
    "Now let’s split our data into training and validation sets. We will use 20% for validation. Again, we will batch the data so it will train more efficiently.\n",
    "```python\n",
    "data = data.shuffle(buffer_size = len(data), reshuffle_each_iteration=False)\n",
    "train_size = int(0.8 * len(data)) # allocate 80% of data for training\n",
    "\n",
    "train = data.take(train_size) # set training data\n",
    "val = data.skip(train_size) # set validation data\n",
    "```\n",
    "```python\n",
    "# check the size of training, validation, testing set\n",
    "len(train), len(val)\n",
    "```\n",
    "```\n",
    "(180, 45)\n",
    "```\n",
    "Here we see that the training data is four times larger than the validation data, which is what we expect.\n",
    "\n",
    "## Text Vectorization\n",
    "In the code below, we standardize the the data by turning all text to lowercase and removing punctuation. We also vectorize the text through creating a text vectorization layer.\n",
    "```python\n",
    "#preparing a text vectorization layer for tf model\n",
    "size_vocabulary = 2000\n",
    "\n",
    "def standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    no_punctuation = tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation),'')\n",
    "    return no_punctuation \n",
    "\n",
    "title_vectorize_layer = TextVectorization(\n",
    "    standardize=standardization,\n",
    "    max_tokens=size_vocabulary, # only consider this many words\n",
    "    output_mode='int',\n",
    "    output_sequence_length=500) \n",
    "\n",
    "title_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n",
    "```\n",
    "```python\n",
    "# Define the shared embedding layer\n",
    "embedding_layer = layers.Embedding(size_vocabulary, 20, name = \"embedding\")\n",
    "\n",
    "# Define the input layers\n",
    "title_input = keras.Input(\n",
    "    shape = (1,), \n",
    "    # name for us to remember for later\n",
    "    name = \"title\",\n",
    "    # type of data contained\n",
    "    dtype = \"string\"\n",
    ")\n",
    "\n",
    "text_input = keras.Input(\n",
    "    shape = (1,), \n",
    "    name = \"text\",\n",
    "    dtype = \"string\"\n",
    ")\n",
    "```\n",
    "## Create models\n",
    "### Model 1: Only using article title as input\n",
    "The first model will determine whether an article contains fake news or not solely based on its title. To do so, we will use the functional API of TensorFlow. We will define a pipeline of hidden layers to process the titles. First, we explicitly define an embedding layer. This is so we can reuse it in a later model. Then we define different layers for the text data. The dropout layers prevent overfitting.\n",
    "\n",
    "```python\n",
    "# Model 1: Only article title as input\n",
    "title_features = title_vectorize_layer(title_input)\n",
    "title_features = embedding_layer(title_features)\n",
    "title_features = layers.Dropout(0.2)(title_features)\n",
    "title_features = layers.GlobalAveragePooling1D()(title_features)\n",
    "title_features = layers.Dropout(0.2)(title_features)\n",
    "title_features = layers.Dense(32, activation='relu')(title_features)\n",
    "\n",
    "title_output = layers.Dense(2,name='fake')(title_features)\n",
    "model_title = keras.Model(\n",
    "    inputs = title_input,\n",
    "    outputs = title_output\n",
    ")\n",
    "model_title.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "```python\n",
    "history_title = model_title.fit(train,\n",
    "                    validation_data=val,\n",
    "                    epochs = 20)\n",
    "```\n",
    "```\n",
    "Epoch 1/20\n",
    "/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n",
    "  inputs = self._flatten_to_reference_inputs(inputs)\n",
    "180/180 [==============================] - 13s 68ms/step - loss: 0.6915 - accuracy: 0.5247 - val_loss: 0.6897 - val_accuracy: 0.5227\n",
    "Epoch 2/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5882 - val_loss: 0.6509 - val_accuracy: 0.6702\n",
    "Epoch 3/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.5618 - accuracy: 0.7883 - val_loss: 0.4337 - val_accuracy: 0.8713\n",
    "Epoch 4/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.3614 - accuracy: 0.8743 - val_loss: 0.2885 - val_accuracy: 0.8998\n",
    "Epoch 5/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.2610 - accuracy: 0.9046 - val_loss: 0.2261 - val_accuracy: 0.9140\n",
    "Epoch 6/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.2151 - accuracy: 0.9202 - val_loss: 0.1936 - val_accuracy: 0.9240\n",
    "Epoch 7/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1854 - accuracy: 0.9296 - val_loss: 0.1745 - val_accuracy: 0.9304\n",
    "Epoch 8/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.1670 - accuracy: 0.9356 - val_loss: 0.1630 - val_accuracy: 0.9351\n",
    "Epoch 9/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1555 - accuracy: 0.9401 - val_loss: 0.1536 - val_accuracy: 0.9396\n",
    "Epoch 10/20\n",
    "180/180 [==============================] - 1s 7ms/step - loss: 0.1447 - accuracy: 0.9444 - val_loss: 0.1477 - val_accuracy: 0.9429\n",
    "Epoch 11/20\n",
    "180/180 [==============================] - 1s 8ms/step - loss: 0.1378 - accuracy: 0.9467 - val_loss: 0.1440 - val_accuracy: 0.9429\n",
    "Epoch 12/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.1319 - accuracy: 0.9484 - val_loss: 0.1417 - val_accuracy: 0.9438\n",
    "Epoch 13/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1265 - accuracy: 0.9504 - val_loss: 0.1399 - val_accuracy: 0.9440\n",
    "Epoch 14/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1228 - accuracy: 0.9522 - val_loss: 0.1377 - val_accuracy: 0.9458\n",
    "Epoch 15/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.1183 - accuracy: 0.9545 - val_loss: 0.1368 - val_accuracy: 0.9464\n",
    "Epoch 16/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.1153 - accuracy: 0.9559 - val_loss: 0.1373 - val_accuracy: 0.9449\n",
    "Epoch 17/20\n",
    "180/180 [==============================] - 1s 5ms/step - loss: 0.1129 - accuracy: 0.9570 - val_loss: 0.1361 - val_accuracy: 0.9469\n",
    "Epoch 18/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1110 - accuracy: 0.9567 - val_loss: 0.1361 - val_accuracy: 0.9484\n",
    "Epoch 19/20\n",
    "180/180 [==============================] - 1s 6ms/step - loss: 0.1086 - accuracy: 0.9584 - val_loss: 0.1370 - val_accuracy: 0.9478\n",
    "Epoch 20/20\n",
    "180/180 [==============================] - 2s 8ms/step - loss: 0.1068 - accuracy: 0.9594 - val_loss: 0.1367 - val_accuracy: 0.9489\n",
    "```\n",
    "```python\n",
    "plt.plot(history_title.history[\"accuracy\"], label = \"training\")\n",
    "plt.plot(history_title.history[\"val_accuracy\"], label = \"validation\")\n",
    "plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n",
    "plt.legend()\n",
    "```\n",
    "![output](hw6-model1.png)\n",
    "We can see that the model achieves around 94% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n",
    "\n",
    "### Model 2: Only using articles text as input\n",
    "We follow the same procedure as the first model to determine whether an article contains fake news or not solely based on its text. \n",
    "```python\n",
    "# Model 2: Only article text as input\n",
    "\n",
    "text_features = title_vectorize_layer(text_input) \n",
    "text_features = embedding_layer(text_features) \n",
    "text_features = layers.Dropout(0.2)(text_features)\n",
    "text_features = layers.GlobalAveragePooling1D()(text_features)\n",
    "text_features = layers.Dropout(0.2)(text_features)\n",
    "text_features = layers.Dense(32, activation='relu')(text_features)\n",
    "\n",
    "text_output = layers.Dense(2,name='fake')(text_features)\n",
    "model_text = keras.Model(\n",
    "    inputs = text_input,\n",
    "    outputs = text_output\n",
    ")\n",
    "model_text.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "```python\n",
    "history_text = model_text.fit(train,\n",
    "                    validation_data=val,\n",
    "                    epochs = 20)\n",
    "```\n",
    "```\n",
    "Epoch 1/20\n",
    "180/180 [==============================] - 11s 54ms/step - loss: 0.4063 - accuracy: 0.9445 - val_loss: 0.1904 - val_accuracy: 0.9700\n",
    "Epoch 2/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.1296 - accuracy: 0.9744 - val_loss: 0.1168 - val_accuracy: 0.9713\n",
    "Epoch 3/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0816 - accuracy: 0.9815 - val_loss: 0.1017 - val_accuracy: 0.9704\n",
    "Epoch 4/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0629 - accuracy: 0.9865 - val_loss: 0.0979 - val_accuracy: 0.9702\n",
    "Epoch 5/20\n",
    "180/180 [==============================] - 3s 15ms/step - loss: 0.0523 - accuracy: 0.9880 - val_loss: 0.0977 - val_accuracy: 0.9698\n",
    "Epoch 6/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0453 - accuracy: 0.9895 - val_loss: 0.1008 - val_accuracy: 0.9700\n",
    "Epoch 7/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0409 - accuracy: 0.9908 - val_loss: 0.1031 - val_accuracy: 0.9691\n",
    "Epoch 8/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0372 - accuracy: 0.9913 - val_loss: 0.1067 - val_accuracy: 0.9691\n",
    "Epoch 9/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0351 - accuracy: 0.9925 - val_loss: 0.1086 - val_accuracy: 0.9700\n",
    "Epoch 10/20\n",
    "180/180 [==============================] - 3s 15ms/step - loss: 0.0321 - accuracy: 0.9933 - val_loss: 0.1115 - val_accuracy: 0.9698\n",
    "Epoch 11/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0313 - accuracy: 0.9931 - val_loss: 0.1141 - val_accuracy: 0.9711\n",
    "Epoch 12/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0294 - accuracy: 0.9935 - val_loss: 0.1177 - val_accuracy: 0.9696\n",
    "Epoch 13/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0279 - accuracy: 0.9938 - val_loss: 0.1190 - val_accuracy: 0.9704\n",
    "Epoch 14/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0273 - accuracy: 0.9937 - val_loss: 0.1211 - val_accuracy: 0.9693\n",
    "Epoch 15/20\n",
    "180/180 [==============================] - 3s 15ms/step - loss: 0.0254 - accuracy: 0.9943 - val_loss: 0.1219 - val_accuracy: 0.9713\n",
    "Epoch 16/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.1260 - val_accuracy: 0.9713\n",
    "Epoch 17/20\n",
    "180/180 [==============================] - 2s 12ms/step - loss: 0.0249 - accuracy: 0.9946 - val_loss: 0.1266 - val_accuracy: 0.9709\n",
    "Epoch 18/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0235 - accuracy: 0.9948 - val_loss: 0.1299 - val_accuracy: 0.9704\n",
    "Epoch 19/20\n",
    "180/180 [==============================] - 2s 11ms/step - loss: 0.0228 - accuracy: 0.9941 - val_loss: 0.1310 - val_accuracy: 0.9709\n",
    "Epoch 20/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0217 - accuracy: 0.9952 - val_loss: 0.1326 - val_accuracy: 0.9716\n",
    "```\n",
    "```python\n",
    "plt.plot(history_text.history[\"accuracy\"], label = \"training\")\n",
    "plt.plot(history_text.history[\"val_accuracy\"], label = \"validation\")\n",
    "plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n",
    "plt.legend()\n",
    "```\n",
    "![output](hw6-model2.png)\n",
    "We can see that the model achieves around 97% validation accuracy. We observe a bit of overfitting as the training data is about 0.2 more than the validation data.\n",
    "\n",
    "### Model 3: Using both the article title and the article text as input\n",
    "In the last model, we will use both the title and text of the article to determine whether the article contains fake news. Since we have already defined the layers for title_features and text_features, we can combine them to create our new model.\n",
    "```python\n",
    "main = layers.concatenate([title_features, text_features], axis = 1) # combine the layers of title and text\n",
    "\n",
    "main = layers.Dense(32, activation = 'relu')(main)\n",
    "output = layers.Dense(2, name = \"fake\")(main)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs = [title_input, text_input],\n",
    "    outputs = output\n",
    ")\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "```python\n",
    "history = model.fit(train,\n",
    "                    validation_data=val,\n",
    "                    epochs = 20)\n",
    "```\n",
    "```\n",
    "Epoch 1/20\n",
    "180/180 [==============================] - 15s 71ms/step - loss: 0.1109 - accuracy: 0.9648 - val_loss: 0.1008 - val_accuracy: 0.9727\n",
    "Epoch 2/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0978 - val_accuracy: 0.9747\n",
    "Epoch 3/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.1065 - val_accuracy: 0.9736\n",
    "Epoch 4/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.1081 - val_accuracy: 0.9729\n",
    "Epoch 5/20\n",
    "180/180 [==============================] - 2s 12ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.1007 - val_accuracy: 0.9744\n",
    "Epoch 6/20\n",
    "180/180 [==============================] - 3s 17ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0988 - val_accuracy: 0.9762\n",
    "Epoch 7/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.1010 - val_accuracy: 0.9771\n",
    "Epoch 8/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0934 - val_accuracy: 0.9767\n",
    "Epoch 9/20\n",
    "180/180 [==============================] - 3s 14ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0920 - val_accuracy: 0.9778\n",
    "Epoch 10/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0909 - val_accuracy: 0.9780\n",
    "Epoch 11/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0878 - val_accuracy: 0.9778\n",
    "Epoch 12/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0941 - val_accuracy: 0.9764\n",
    "Epoch 13/20\n",
    "180/180 [==============================] - 3s 16ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0945 - val_accuracy: 0.9773\n",
    "Epoch 14/20\n",
    "180/180 [==============================] - 3s 14ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0940 - val_accuracy: 0.9778\n",
    "Epoch 15/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0892 - val_accuracy: 0.9782\n",
    "Epoch 16/20\n",
    "180/180 [==============================] - 3s 14ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0917 - val_accuracy: 0.9778\n",
    "Epoch 17/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0984 - val_accuracy: 0.9776\n",
    "Epoch 18/20\n",
    "180/180 [==============================] - 3s 17ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.1054 - val_accuracy: 0.9753\n",
    "Epoch 19/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.0885 - val_accuracy: 0.9796\n",
    "Epoch 20/20\n",
    "180/180 [==============================] - 2s 13ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0948 - val_accuracy: 0.9778\n",
    "```\n",
    "```python\n",
    "plt.plot(history.history[\"accuracy\"], label = \"training\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n",
    "plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n",
    "plt.legend()\n",
    "```\n",
    "![output](hw6-model3.png)\n",
    "We can see that the model achieves around 99% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n",
    "\n",
    "## Model evaluation\n",
    "We can test out our model on unseen test data. The code below shows we have achieved 97.46% accuracy on the test data!\n",
    "```python\n",
    "test_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\n",
    "test_data = pd.read_csv(test_url)\n",
    "test = make_dataset(test_data)\n",
    "\n",
    "model.evaluate(test)\n",
    "```\n",
    "```\n",
    "225/225 [==============================] - 2s 9ms/step - loss: 0.1169 - accuracy: 0.9746\n",
    "[0.1169150322675705, 0.9746091365814209]\n",
    "```\n",
    "\n",
    "## Embedding Visualization\n",
    "We can use plotly to create an interactive plot to see how the words are related to each other. The words that the model associates with fake news will tend towards one side and the words that the model associates with real news will be on the other.\n",
    "```python\n",
    "weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\n",
    "vocab = title_vectorize_layer.get_vocabulary()         # get the vocabulary from our data prep for later\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "weights = pca.fit_transform(weights)\n",
    "\n",
    "embedding_df = pd.DataFrame({\n",
    "    'word' : vocab,\n",
    "    'x0'   : weights[:,0],\n",
    "    'x1'   : weights[:,1]\n",
    "})\n",
    "```\n",
    "```python\n",
    "import plotly.express as px\n",
    "fig = px.scatter(embedding_df,\n",
    "                 x = \"x0\",\n",
    "                 y = \"x1\",\n",
    "                 size = list(np.ones(len(embedding_df))),\n",
    "                 size_max = 3,\n",
    "                 hover_name = \"word\")\n",
    "\n",
    "fig.show()\n",
    "```\n",
    "![output](hw6-visualization.png)\n",
    "On the far left we can see words such as “trumps”, \"obamas\", as well as country names such as “chinas\", \"koreas\". On the right there are words such as \"shocking\", \"reportedly\", \"insane\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c370d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
