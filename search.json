[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "HW1",
    "section": "",
    "text": "First, we will create a database with three tables: temperatures, stations, and countries. Lets import the necessary libraries.\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nNow, we will read the temperatures data from a .csv file. Because this file is large, we read it into the database in chunks of 100000 rows. We will use the prepare_df function to clean up the data chunks as we read it in.\nconn = sqlite3.connect(\"hw1.db\") #open database connection\n\ntemps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\nfor temps in temps_iter:\n    temps = prepare_df(temps)\n    temps.to_sql(\"temperatures\", conn, if_exists = \"append\", index = False)\ntemps.head()\nLet’s read the data for countries and stations next.\ncountries = pd.read_csv('country-names.csv')\ncountries = countries.rename(columns= {\"FIPS 10-4\": \"FIPS_10-4\", \"ISO 3166\": \"ISO_3166\"})\ncountries.head()\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(url)\nstations[\"FIPS_10-4\"] = stations[\"ID\"].str[0:2]\nstations.head()\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n\n\nWe can write a function that uses a SQL command to extract a Pandas dataframe from our database. We use the LEFT JOIN keyword to merge the three tables, and the WHERE to specify the conditions (a country, two integers that give the earliest and latest years, and a specific month).\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \n    conn = sqlite3.connect(db_file)\n\n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C on SUBSTRING (T.id, 1, 2) = C.'FIPS 10-4'\n    WHERE C.name = ? AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month}\n    \n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn, params=(country,))\n    conn.close()\n\n    df = df.rename(columns={'Name': 'Country'})\n\n    return df\n\nHere’s an example of using the function query_climate_database to query temperature data in India for August from 1980-2020.\n```python\nquery_climate_database(db_file = \"hw1.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\noutput\n\n\n\n\n\n\n\nThis visualization answers the question: How does the average yearly change in temperature vary within a given country?\nWe will first import the packages necessary to achieve this. sklearn is used for linear regression, datetime converts numbers to their corresponding month names, and plotly is used to create the interactive visualizations.\nfrom sklearn.linear_model import LinearRegression\nimport datetime\nfrom plotly import express as px\nNow, we can define a function to perform a linear regression of the temperature on the year, and return the coefficients.\ndef coef(data_group):\n    '''\n    Inputs: dataframe\n    Returns: slope of linear model, representing the average yearly change in temperature\n    '''\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\nThe function temperature_coefficient_plot queries the specified data and produces a geographic scatterplot. The location of each point is the location of the station and the color is based on the estimate of yearly change in temperature at the station in the given time interval.\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    Inputs: a country, a bounded time interval, a month, minimum data points to be considered, and optional\n    plotting arguments\n    Returns: a geographic plot indicating the changes in temperature over time\n    '''\n    #obtain and clean the data\n    df = query_climate_database(db_file = \"hw1.db\", country = country, year_begin = year_begin, year_end = year_end, month = month) #read in the data using previously defined function\n    counts = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform(len)\n    df = df[counts &gt;= min_obs]\n    coefs = df.groupby([\"NAME\", \"Month\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef) #find the estimated yearly change in temperature for each station\n    coefs = coefs.round(3) # round data to 3 decimal places\n    coefs = coefs.reset_index()\n    coefs = coefs.rename(columns = {0 : \"Estimated Yearly Change (C)\"})\n\n    #create the plot\n    title = \"Estimates of Yearly Increase in Temperature in {a} for stations in {b}, years {c} - {d}\"\\\n    .format(a=datetime.date(2021, month, 1).strftime('%B'), b=country, c=year_begin, d=year_end)\n    fig = px.scatter_mapbox(coefs,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Estimated Yearly Change (C)\",\n                            title = title,\n                            **kwargs)\n    return fig\nHere is an example of using the temperature_coefficient_plot function to visualize the estimated change in temperature across stations in India in January from 1980 to 2020.\ncolor_map = px.colors.diverging.RdGy_r \n\nfig = temperature_coefficient_plot(country = \"India\", year_begin = 1980, year_end = 2020, month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\noutput\n\n\n\n\n\nThe visualization I chose answers the question: How is the mean temperature per year changing in comparison to the overall mean temperature?\nThe function diff_from_mean_temp function takes in the same arguments as the query_climate_database: country, year_begin, year_end, and month. With the returned dataframe, it takes the mean temperature over the entire time period and then finds the difference between the mean temperature of each year and the overall mean temperature.\ndef diff_from_mean_temp(country, year_begin, year_end, month, **kwargs):\n    '''\n    Input: a country, a bounded time interval, and a month\n    Returns: a barplot comparing the yearly temperature to the average temperature over the interval\n    '''\n    df = query_climate_database(\"hw1.db\", country, year_begin, year_end, month) \n    mean = np.mean(df[\"Temp\"]) #overall mean temperature of the time interval\n    df = df.groupby([\"Year\"])[\"Temp\"].aggregate(np.mean) #find mean temperature per year\n    df = df.reset_index()\n    df[\"Difference (C)\"] = df[\"Temp\"] - mean #compare mean temperature of year to mean temperature over entire time interval\n    df= df.round(3) #round to 3 decimal places\n    df[\"col\"] = np.where(df[\"Difference (C)\"]&gt;=0, 'red', 'blue')\n\n    #create the plot\n    title = \"Difference in Mean Temperature in {a} for stations in {b} from {c} to {d}\"\\\n    .format(a=datetime.date(2021, month, 1).strftime('%B'), b=country, c=year_begin, d=year_end)\n    fig = px.bar(df, x = \"Year\", y = \"Difference (C)\",\n                hover_data = [\"Year\", \"Difference (C)\"], title = title, color = \"col\", **kwargs)\n    fig.update_layout(showlegend=False) \n    return fig\nHere is an example of using the diff_from_mean_temp function to visualize the difference in mean temperature in China in March from 1980 to 2020.\nfig = diff_from_mean_temp(\"China\", 1980, 2020, 3)\nfig.show()\n Here, the positive red bars indicate that the temperature readings that year were above the mean, and negative blue bars indicate that temperature readings that year were below the mean.\n\n\n\nThe visualization I chose answers the question: How is does temperature vary within regions of a country in a given year?\nThe function query_country_year extracts the relevant data, and the output is passed to year_diff to calculate the difference between the average temperature in the warmest month and the coldest month, at the station level.\ndef query_country_year(db_file, country, year):\n    '''\n    Returns: a dataframe of all temperatures in a country in a single year\n    '''\n    conn = sqlite3.connect(db_file)\n    #cursor = conn.cursor()\n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON C.'FIPS_10-4' = S.'FIPS_10-4'\n    WHERE T.year = {year} AND C.Name = '{country}'\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    return df\ndef year_diff(df):\n    '''\n    Computes temperature difference between warmest and coldest month\n    '''\n    df_group = df.groupby([\"NAME\"])\n    obs_filter = df_group.count()[\"Name\"]&gt;=12\n    temp_diff = (df_group.max()[['Temp']] - df_group.min()[['Temp']])[obs_filter]\n    coords = df.groupby([\"NAME\"]).first()[[\"LATITUDE\",\"LONGITUDE\"]][obs_filter]\n    x = pd.merge(temp_diff,coords,on='NAME')\n    x = x.rename(columns={'Temp':\"Temperature difference\"})\n    x = x.reset_index()\n    return x\nThe function seasonal_difference_plot takes in a country and a year, and creates a scatterplot of the difference in temperature between the warmest and coolest month of that year.\ndef seasonal_difference_plot(country, year,**kwargs):\n    x = year_diff(query_country_year(\"hw1.db\", country, year))\n    title = f\"Difference in temperature between warmest and coolest month in {year} for stations in {country}\"\n    fig = px.scatter_mapbox(x,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Temperature difference\",\n                            hover_data={\n                                \"LATITUDE\":':.3f',\n                                \"LONGITUDE\":':.3f',\n                                \"Temperature difference\":':.3f'\n                            },\n                            title = title,\n                            color_continuous_scale=color_map,\n                            **kwargs)\n\n    return fig\nAn example can be seen with Mexico in the year 2020.\nfig = seasonal_difference_plot('Mexico', 2020,\n                               mapbox_style=\"carto-positron\",\n                               zoom=1)\npio.write_image(fig, \"seasonal-diff-barplot.png\")\nfig.show()\n\n\n\noutput\n\n\n\n\n\nThe last visualization I chose answers the question: How does temperature vary across months in a given country? The function query_year_range extracts the relevant data and creates a pandas dataframe from our database. We use the LEFT JOIN keyword to merge the three tables, and the WHERE to specify the conditions (a country and two integers that give the earliest and latest years).\ndef query_country_year_range(db_file, country, year_begin, year_end):\n    '''\n    returns dataframe of all temperatures in a country in a single year.\n    '''\n    conn = sqlite3.connect(db_file)\n    \n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON C.'FIPS 10-4' = S.'FIPS 10-4'\n    WHERE C.name = ? AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end}\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn, params=(country,))\n    return df\nThe function facet_hist takes in a country and two integers for years, and creates a faceted histogram of the temperatures for each month.\ndef facet_hist(country, year_begin, year_end, **kwargs):\n    '''\n    Inputs:\n    '''\n    df = query_country_year_range(\"hw1.db\", country, year_begin, year_end) \n    df.sort_values([\"Year\", \"Month\", \"Temp\"], inplace = True)\n    \n    title = f\"Distribution of Monthly Temperature in {country} between {year_begin} and {year_end}\"\n    fig = px.histogram(df,\n                       x = \"Temp\", \n                       color = \"Year\",\n                       opacity = 0.8, \n                       nbins = 20,\n                       barmode='group',\n                       histnorm = 'percent',\n                       width = 900,\n                       height = 1500,\n                       facet_col = 'Month',\n                       facet_col_wrap = 3, \n                       title = title,\n                        **kwargs)\n    return fig\nAn example can be seen with Canada in the 2018-2020.\nfig = facet_hist(\"Canada\", 2018, 2020)\nfig.show()\n\n\n\noutput"
  },
  {
    "objectID": "posts/hw1/index.html#create-a-database",
    "href": "posts/hw1/index.html#create-a-database",
    "title": "HW1",
    "section": "",
    "text": "First, we will create a database with three tables: temperatures, stations, and countries. Lets import the necessary libraries.\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nNow, we will read the temperatures data from a .csv file. Because this file is large, we read it into the database in chunks of 100000 rows. We will use the prepare_df function to clean up the data chunks as we read it in.\nconn = sqlite3.connect(\"hw1.db\") #open database connection\n\ntemps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\nfor temps in temps_iter:\n    temps = prepare_df(temps)\n    temps.to_sql(\"temperatures\", conn, if_exists = \"append\", index = False)\ntemps.head()\nLet’s read the data for countries and stations next.\ncountries = pd.read_csv('country-names.csv')\ncountries = countries.rename(columns= {\"FIPS 10-4\": \"FIPS_10-4\", \"ISO 3166\": \"ISO_3166\"})\ncountries.head()\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(url)\nstations[\"FIPS_10-4\"] = stations[\"ID\"].str[0:2]\nstations.head()\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)"
  },
  {
    "objectID": "posts/hw1/index.html#query-the-database",
    "href": "posts/hw1/index.html#query-the-database",
    "title": "HW1",
    "section": "",
    "text": "We can write a function that uses a SQL command to extract a Pandas dataframe from our database. We use the LEFT JOIN keyword to merge the three tables, and the WHERE to specify the conditions (a country, two integers that give the earliest and latest years, and a specific month).\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \n    conn = sqlite3.connect(db_file)\n\n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C on SUBSTRING (T.id, 1, 2) = C.'FIPS 10-4'\n    WHERE C.name = ? AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month}\n    \n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn, params=(country,))\n    conn.close()\n\n    df = df.rename(columns={'Name': 'Country'})\n\n    return df\n\nHere’s an example of using the function query_climate_database to query temperature data in India for August from 1980-2020.\n```python\nquery_climate_database(db_file = \"hw1.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\noutput"
  },
  {
    "objectID": "posts/hw1/index.html#creating-interactive-visualizations",
    "href": "posts/hw1/index.html#creating-interactive-visualizations",
    "title": "HW1",
    "section": "",
    "text": "This visualization answers the question: How does the average yearly change in temperature vary within a given country?\nWe will first import the packages necessary to achieve this. sklearn is used for linear regression, datetime converts numbers to their corresponding month names, and plotly is used to create the interactive visualizations.\nfrom sklearn.linear_model import LinearRegression\nimport datetime\nfrom plotly import express as px\nNow, we can define a function to perform a linear regression of the temperature on the year, and return the coefficients.\ndef coef(data_group):\n    '''\n    Inputs: dataframe\n    Returns: slope of linear model, representing the average yearly change in temperature\n    '''\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\nThe function temperature_coefficient_plot queries the specified data and produces a geographic scatterplot. The location of each point is the location of the station and the color is based on the estimate of yearly change in temperature at the station in the given time interval.\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    Inputs: a country, a bounded time interval, a month, minimum data points to be considered, and optional\n    plotting arguments\n    Returns: a geographic plot indicating the changes in temperature over time\n    '''\n    #obtain and clean the data\n    df = query_climate_database(db_file = \"hw1.db\", country = country, year_begin = year_begin, year_end = year_end, month = month) #read in the data using previously defined function\n    counts = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform(len)\n    df = df[counts &gt;= min_obs]\n    coefs = df.groupby([\"NAME\", \"Month\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef) #find the estimated yearly change in temperature for each station\n    coefs = coefs.round(3) # round data to 3 decimal places\n    coefs = coefs.reset_index()\n    coefs = coefs.rename(columns = {0 : \"Estimated Yearly Change (C)\"})\n\n    #create the plot\n    title = \"Estimates of Yearly Increase in Temperature in {a} for stations in {b}, years {c} - {d}\"\\\n    .format(a=datetime.date(2021, month, 1).strftime('%B'), b=country, c=year_begin, d=year_end)\n    fig = px.scatter_mapbox(coefs,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Estimated Yearly Change (C)\",\n                            title = title,\n                            **kwargs)\n    return fig\nHere is an example of using the temperature_coefficient_plot function to visualize the estimated change in temperature across stations in India in January from 1980 to 2020.\ncolor_map = px.colors.diverging.RdGy_r \n\nfig = temperature_coefficient_plot(country = \"India\", year_begin = 1980, year_end = 2020, month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\noutput\n\n\n\n\n\nThe visualization I chose answers the question: How is the mean temperature per year changing in comparison to the overall mean temperature?\nThe function diff_from_mean_temp function takes in the same arguments as the query_climate_database: country, year_begin, year_end, and month. With the returned dataframe, it takes the mean temperature over the entire time period and then finds the difference between the mean temperature of each year and the overall mean temperature.\ndef diff_from_mean_temp(country, year_begin, year_end, month, **kwargs):\n    '''\n    Input: a country, a bounded time interval, and a month\n    Returns: a barplot comparing the yearly temperature to the average temperature over the interval\n    '''\n    df = query_climate_database(\"hw1.db\", country, year_begin, year_end, month) \n    mean = np.mean(df[\"Temp\"]) #overall mean temperature of the time interval\n    df = df.groupby([\"Year\"])[\"Temp\"].aggregate(np.mean) #find mean temperature per year\n    df = df.reset_index()\n    df[\"Difference (C)\"] = df[\"Temp\"] - mean #compare mean temperature of year to mean temperature over entire time interval\n    df= df.round(3) #round to 3 decimal places\n    df[\"col\"] = np.where(df[\"Difference (C)\"]&gt;=0, 'red', 'blue')\n\n    #create the plot\n    title = \"Difference in Mean Temperature in {a} for stations in {b} from {c} to {d}\"\\\n    .format(a=datetime.date(2021, month, 1).strftime('%B'), b=country, c=year_begin, d=year_end)\n    fig = px.bar(df, x = \"Year\", y = \"Difference (C)\",\n                hover_data = [\"Year\", \"Difference (C)\"], title = title, color = \"col\", **kwargs)\n    fig.update_layout(showlegend=False) \n    return fig\nHere is an example of using the diff_from_mean_temp function to visualize the difference in mean temperature in China in March from 1980 to 2020.\nfig = diff_from_mean_temp(\"China\", 1980, 2020, 3)\nfig.show()\n Here, the positive red bars indicate that the temperature readings that year were above the mean, and negative blue bars indicate that temperature readings that year were below the mean.\n\n\n\nThe visualization I chose answers the question: How is does temperature vary within regions of a country in a given year?\nThe function query_country_year extracts the relevant data, and the output is passed to year_diff to calculate the difference between the average temperature in the warmest month and the coldest month, at the station level.\ndef query_country_year(db_file, country, year):\n    '''\n    Returns: a dataframe of all temperatures in a country in a single year\n    '''\n    conn = sqlite3.connect(db_file)\n    #cursor = conn.cursor()\n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON C.'FIPS_10-4' = S.'FIPS_10-4'\n    WHERE T.year = {year} AND C.Name = '{country}'\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    return df\ndef year_diff(df):\n    '''\n    Computes temperature difference between warmest and coldest month\n    '''\n    df_group = df.groupby([\"NAME\"])\n    obs_filter = df_group.count()[\"Name\"]&gt;=12\n    temp_diff = (df_group.max()[['Temp']] - df_group.min()[['Temp']])[obs_filter]\n    coords = df.groupby([\"NAME\"]).first()[[\"LATITUDE\",\"LONGITUDE\"]][obs_filter]\n    x = pd.merge(temp_diff,coords,on='NAME')\n    x = x.rename(columns={'Temp':\"Temperature difference\"})\n    x = x.reset_index()\n    return x\nThe function seasonal_difference_plot takes in a country and a year, and creates a scatterplot of the difference in temperature between the warmest and coolest month of that year.\ndef seasonal_difference_plot(country, year,**kwargs):\n    x = year_diff(query_country_year(\"hw1.db\", country, year))\n    title = f\"Difference in temperature between warmest and coolest month in {year} for stations in {country}\"\n    fig = px.scatter_mapbox(x,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Temperature difference\",\n                            hover_data={\n                                \"LATITUDE\":':.3f',\n                                \"LONGITUDE\":':.3f',\n                                \"Temperature difference\":':.3f'\n                            },\n                            title = title,\n                            color_continuous_scale=color_map,\n                            **kwargs)\n\n    return fig\nAn example can be seen with Mexico in the year 2020.\nfig = seasonal_difference_plot('Mexico', 2020,\n                               mapbox_style=\"carto-positron\",\n                               zoom=1)\npio.write_image(fig, \"seasonal-diff-barplot.png\")\nfig.show()\n\n\n\noutput\n\n\n\n\n\nThe last visualization I chose answers the question: How does temperature vary across months in a given country? The function query_year_range extracts the relevant data and creates a pandas dataframe from our database. We use the LEFT JOIN keyword to merge the three tables, and the WHERE to specify the conditions (a country and two integers that give the earliest and latest years).\ndef query_country_year_range(db_file, country, year_begin, year_end):\n    '''\n    returns dataframe of all temperatures in a country in a single year.\n    '''\n    conn = sqlite3.connect(db_file)\n    \n    cmd = f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON C.'FIPS 10-4' = S.'FIPS 10-4'\n    WHERE C.name = ? AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end}\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn, params=(country,))\n    return df\nThe function facet_hist takes in a country and two integers for years, and creates a faceted histogram of the temperatures for each month.\ndef facet_hist(country, year_begin, year_end, **kwargs):\n    '''\n    Inputs:\n    '''\n    df = query_country_year_range(\"hw1.db\", country, year_begin, year_end) \n    df.sort_values([\"Year\", \"Month\", \"Temp\"], inplace = True)\n    \n    title = f\"Distribution of Monthly Temperature in {country} between {year_begin} and {year_end}\"\n    fig = px.histogram(df,\n                       x = \"Temp\", \n                       color = \"Year\",\n                       opacity = 0.8, \n                       nbins = 20,\n                       barmode='group',\n                       histnorm = 'percent',\n                       width = 900,\n                       height = 1500,\n                       facet_col = 'Month',\n                       facet_col_wrap = 3, \n                       title = title,\n                        **kwargs)\n    return fig\nAn example can be seen with Canada in the 2018-2020.\nfig = facet_hist(\"Canada\", 2018, 2020)\nfig.show()\n\n\n\noutput"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "HW4",
    "section": "",
    "text": "In the homework, we will be conducting a simulation of two-dimensional heat diffusion in various ways. First, let’s define our initial conditions and import the necessary libraries.\nN = 101\nepsilon = 0.2\niterations = 2700\nplot_interval = 300\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\nHere, we have the initial condition as in the 1D case: putting 1 unit of heat at the midpoint. \n\n\nFirst, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution.\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\nThis function takes the value N as the argument and returns the corresponding matrix A.\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\nAfter defining advance_time_matvecmul and get_A, we can run the simulation to visualize the diffusion of heat every 300 iterations using a heatmap.\nstart_time = time.time() #start time\n# Initialize intermediate solutions array for visualization\nintermediate_solutions = []\n\n# Construct finite difference matrix\nA = get_A(N)\n\n# Run simulation\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_matvecmul(A, current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions.append(current_solution)\n\nend_time = time.time() # end time excluding time for visualization\n        \n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\nelapsed_time = end_time - start_time\nprint(elapsed_time) #44.71136999130249\nWe can see that using matrix multiplication is quite slow, and takes 44.71136999130249 seconds to generate the heat diffusion simulation below. \n\n\n\n\n\n\nA much efficient way to generate this simulation is through direct operation with numpy. Here, we can define advance_time_numpy in heat_equation.py and run the simulation in intervals similar to the other methods.\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the solution by one timestep using numpy vectorized operations.\n    \n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad zeros to form an (N+2) x (N+2) array\n    padded_u = np.pad(u, 1, mode='constant')\n\n    # Compute the Laplacian using np.roll()\n    laplacian = (\n        np.roll(padded_u, 1, axis=0) + np.roll(padded_u, -1, axis=0) +\n        np.roll(padded_u, 1, axis=1) + np.roll(padded_u, -1, axis=1) -\n        4 * padded_u\n    )\n\n    # Update the solution using the heat equation\n    new_u = u + epsilon * laplacian[1:-1, 1:-1]\n\n    return new_u\nstart_time = time.time()\n\n# Construct initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Initialize intermediate solutions array for visualization\nintermediate_solutions_numpy = []\n\n# Run simulation with numpy\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_numpy(current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions_numpy.append(current_solution)\n\nend_time = time.time()\n\n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions_numpy):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show() \nelapsed_time = end_time - start_time\nprint(elapsed_time) #0.18757390975952148\nWe can see that using direct operation with numpy is significantly faster, only taking 0.18757390975952148 seconds to run the simulation. \n\n\n\nFinally, we can use JAX and define advance_time_jax without using a sparse matrix.\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the solution by one timestep using JAX and JIT compilation\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # Extract the size of the grid\n    N = u.shape[0]\n    \n    # Create a padded version of 'u' to simplify boundary computations\n    padded_u = jnp.pad(u, 1, mode='constant')\n    \n    # Calculate the updates in a vectorized manner\n    update_value = epsilon * (padded_u[:-2, 1:-1] + padded_u[2:, 1:-1] + padded_u[1:-1, :-2] + padded_u[1:-1, 2:] - 4 * u)\n    u_new = u + update_value\n    \n    return u_new\nstart_time = time.time()\n\n# Construct initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Initialize intermediate solutions array for visualization\nintermediate_solutions_jax = []\n\n# Run simulation with JAX\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_jax(current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions_jax.append(current_solution)\n\nend_time = time.time()\n\n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions_jax):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show() \nelapsed_time = end_time - start_time\nprint(elapsed_time) #0.07134509086608887\nUsing JAX only takes us 0.07134509086608887 seconds to generate the simulation. This method is the fastest, and has clean and short code that is easy to follow. \n\n\n\nThe full implementation for heat_equation.py can be seen below.\nN = 101\nepsilon = 0.2\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax.numpy as jnp\nimport jax\nfrom jax import jit, ops\nfrom jax import lax\nfrom jax.experimental.sparse import bcoo\nimport jax.scipy.sparse as sps\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n# @jit\ndef advance_time_matvecmul_sparse(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    \n    # Convert u to a vector\n    u_vec = u.flatten()\n    \n    # Sparse matrix-vector multiplication\n    result_vec = bcoo.bcoo_multiply_dense(A, u_vec)\n    \n    # Reshape the result vector to N x N grid\n    result_grid = result_vec.reshape((N, N))\n    \n    # Update the grid state\n    u_new = u + epsilon * result_grid\n    return u_new\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\ndef get_sparse_A(N):\n    \"\"\"Constructs the finite difference matrix A for 2D heat equation in a sparse format using JAX's experimental sparse module.\n    \n    Args:\n        N: Size of the grid.\n        \n    Returns:\n        A_sp_matrix: The sparse representation of finite difference matrix in COO format.\n    \"\"\"\n    n = N * N\n\n    # Initialize data for diagonals\n    data = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n\n    # Modify diagonals\n    data[1] = jnp.where(jnp.arange(1, n) % N == 0, 0, data[1])\n    data[2] = jnp.where(jnp.arange(0, n-1) % N == 0, 0, data[2])\n\n   # rows = jnp.tile(jnp.arange(N), N)\n   # cols = jnp.repeat(jnp.arange(N), N)\n\n    # Create sparse matrix in BCOO format\n   # offsets = [-N, -1, 0, 1, N]\n    A_sp_matrix = bcoo.BCOO(data, shape=(n, n), indices_sorted=True)\n\n    return A_sp_matrix\n'''\ndef get_sparse_A(N):\n    \"\"\"Constructs the finite difference matrix A for 2D heat equation in a sparse format.\n    \n    Args:\n        N: Size of the grid.\n        \n    Returns:\n        A_sp_matrix: The sparse representation of finite difference matrix.\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = jnp.where(jnp.arange(1, n) % N == 0, 0, diagonals[1])\n    diagonals[2] = jnp.where(jnp.arange(0, n-1) % N == 0, 0, diagonals[2])\n    return A_sp_matrix\n'''\n\n# part 3\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the solution by one timestep using numpy vectorized operations.\n    \n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad zeros to form an (N+2) x (N+2) array\n    padded_u = np.pad(u, 1, mode='constant')\n\n    # Compute the Laplacian using np.roll()\n    laplacian = (\n        np.roll(padded_u, 1, axis=0) + np.roll(padded_u, -1, axis=0) +\n        np.roll(padded_u, 1, axis=1) + np.roll(padded_u, -1, axis=1) -\n        4 * padded_u\n    )\n\n    # Update the solution using the heat equation\n    new_u = u + epsilon * laplacian[1:-1, 1:-1]\n\n    return new_u\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the solution by one timestep using JAX and JIT compilation\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # Extract the size of the grid\n    N = u.shape[0]\n    \n    # Create a padded version of 'u' to simplify boundary computations\n    padded_u = jnp.pad(u, 1, mode='constant')\n    \n    # Calculate the updates in a vectorized manner\n    update_value = epsilon * (padded_u[:-2, 1:-1] + padded_u[2:, 1:-1] + padded_u[1:-1, :-2] + padded_u[1:-1, 2:] - 4 * u)\n    u_new = u + update_value\n    \n    return u_new\n\n\n# JIT compile the function\n#advance_time_jax_jit = jit(advance_time_jax)"
  },
  {
    "objectID": "posts/hw4/index.html#with-matrix-multiplication",
    "href": "posts/hw4/index.html#with-matrix-multiplication",
    "title": "HW4",
    "section": "",
    "text": "First, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution.\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\nThis function takes the value N as the argument and returns the corresponding matrix A.\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\nAfter defining advance_time_matvecmul and get_A, we can run the simulation to visualize the diffusion of heat every 300 iterations using a heatmap.\nstart_time = time.time() #start time\n# Initialize intermediate solutions array for visualization\nintermediate_solutions = []\n\n# Construct finite difference matrix\nA = get_A(N)\n\n# Run simulation\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_matvecmul(A, current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions.append(current_solution)\n\nend_time = time.time() # end time excluding time for visualization\n        \n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\nelapsed_time = end_time - start_time\nprint(elapsed_time) #44.71136999130249\nWe can see that using matrix multiplication is quite slow, and takes 44.71136999130249 seconds to generate the heat diffusion simulation below."
  },
  {
    "objectID": "posts/hw4/index.html#direct-operation-with-numpy",
    "href": "posts/hw4/index.html#direct-operation-with-numpy",
    "title": "HW4",
    "section": "",
    "text": "A much efficient way to generate this simulation is through direct operation with numpy. Here, we can define advance_time_numpy in heat_equation.py and run the simulation in intervals similar to the other methods.\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the solution by one timestep using numpy vectorized operations.\n    \n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad zeros to form an (N+2) x (N+2) array\n    padded_u = np.pad(u, 1, mode='constant')\n\n    # Compute the Laplacian using np.roll()\n    laplacian = (\n        np.roll(padded_u, 1, axis=0) + np.roll(padded_u, -1, axis=0) +\n        np.roll(padded_u, 1, axis=1) + np.roll(padded_u, -1, axis=1) -\n        4 * padded_u\n    )\n\n    # Update the solution using the heat equation\n    new_u = u + epsilon * laplacian[1:-1, 1:-1]\n\n    return new_u\nstart_time = time.time()\n\n# Construct initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Initialize intermediate solutions array for visualization\nintermediate_solutions_numpy = []\n\n# Run simulation with numpy\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_numpy(current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions_numpy.append(current_solution)\n\nend_time = time.time()\n\n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions_numpy):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show() \nelapsed_time = end_time - start_time\nprint(elapsed_time) #0.18757390975952148\nWe can see that using direct operation with numpy is significantly faster, only taking 0.18757390975952148 seconds to run the simulation."
  },
  {
    "objectID": "posts/hw4/index.html#with-jax",
    "href": "posts/hw4/index.html#with-jax",
    "title": "HW4",
    "section": "",
    "text": "Finally, we can use JAX and define advance_time_jax without using a sparse matrix.\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the solution by one timestep using JAX and JIT compilation\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # Extract the size of the grid\n    N = u.shape[0]\n    \n    # Create a padded version of 'u' to simplify boundary computations\n    padded_u = jnp.pad(u, 1, mode='constant')\n    \n    # Calculate the updates in a vectorized manner\n    update_value = epsilon * (padded_u[:-2, 1:-1] + padded_u[2:, 1:-1] + padded_u[1:-1, :-2] + padded_u[1:-1, 2:] - 4 * u)\n    u_new = u + update_value\n    \n    return u_new\nstart_time = time.time()\n\n# Construct initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Initialize intermediate solutions array for visualization\nintermediate_solutions_jax = []\n\n# Run simulation with JAX\ncurrent_solution = u0.copy()\nfor i in range(iterations):\n    current_solution = advance_time_jax(current_solution, epsilon)\n    if (i + 1) % plot_interval == 0:\n        intermediate_solutions_jax.append(current_solution)\n\nend_time = time.time()\n\n# Visualize diffusion at specified intervals\nplt.figure(figsize=(12, 12))\nfor i, solution in enumerate(intermediate_solutions_jax):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(solution, cmap='hot')\n    plt.title(f\"Iteration {(i+1)*plot_interval}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show() \nelapsed_time = end_time - start_time\nprint(elapsed_time) #0.07134509086608887\nUsing JAX only takes us 0.07134509086608887 seconds to generate the simulation. This method is the fastest, and has clean and short code that is easy to follow."
  },
  {
    "objectID": "posts/hw4/index.html#full-code-for-heat_equation.py",
    "href": "posts/hw4/index.html#full-code-for-heat_equation.py",
    "title": "HW4",
    "section": "",
    "text": "The full implementation for heat_equation.py can be seen below.\nN = 101\nepsilon = 0.2\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax.numpy as jnp\nimport jax\nfrom jax import jit, ops\nfrom jax import lax\nfrom jax.experimental.sparse import bcoo\nimport jax.scipy.sparse as sps\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n# @jit\ndef advance_time_matvecmul_sparse(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    \n    # Convert u to a vector\n    u_vec = u.flatten()\n    \n    # Sparse matrix-vector multiplication\n    result_vec = bcoo.bcoo_multiply_dense(A, u_vec)\n    \n    # Reshape the result vector to N x N grid\n    result_grid = result_vec.reshape((N, N))\n    \n    # Update the grid state\n    u_new = u + epsilon * result_grid\n    return u_new\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\ndef get_sparse_A(N):\n    \"\"\"Constructs the finite difference matrix A for 2D heat equation in a sparse format using JAX's experimental sparse module.\n    \n    Args:\n        N: Size of the grid.\n        \n    Returns:\n        A_sp_matrix: The sparse representation of finite difference matrix in COO format.\n    \"\"\"\n    n = N * N\n\n    # Initialize data for diagonals\n    data = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n\n    # Modify diagonals\n    data[1] = jnp.where(jnp.arange(1, n) % N == 0, 0, data[1])\n    data[2] = jnp.where(jnp.arange(0, n-1) % N == 0, 0, data[2])\n\n   # rows = jnp.tile(jnp.arange(N), N)\n   # cols = jnp.repeat(jnp.arange(N), N)\n\n    # Create sparse matrix in BCOO format\n   # offsets = [-N, -1, 0, 1, N]\n    A_sp_matrix = bcoo.BCOO(data, shape=(n, n), indices_sorted=True)\n\n    return A_sp_matrix\n'''\ndef get_sparse_A(N):\n    \"\"\"Constructs the finite difference matrix A for 2D heat equation in a sparse format.\n    \n    Args:\n        N: Size of the grid.\n        \n    Returns:\n        A_sp_matrix: The sparse representation of finite difference matrix.\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = jnp.where(jnp.arange(1, n) % N == 0, 0, diagonals[1])\n    diagonals[2] = jnp.where(jnp.arange(0, n-1) % N == 0, 0, diagonals[2])\n    return A_sp_matrix\n'''\n\n# part 3\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the solution by one timestep using numpy vectorized operations.\n    \n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad zeros to form an (N+2) x (N+2) array\n    padded_u = np.pad(u, 1, mode='constant')\n\n    # Compute the Laplacian using np.roll()\n    laplacian = (\n        np.roll(padded_u, 1, axis=0) + np.roll(padded_u, -1, axis=0) +\n        np.roll(padded_u, 1, axis=1) + np.roll(padded_u, -1, axis=1) -\n        4 * padded_u\n    )\n\n    # Update the solution using the heat equation\n    new_u = u + epsilon * laplacian[1:-1, 1:-1]\n\n    return new_u\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the solution by one timestep using JAX and JIT compilation\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # Extract the size of the grid\n    N = u.shape[0]\n    \n    # Create a padded version of 'u' to simplify boundary computations\n    padded_u = jnp.pad(u, 1, mode='constant')\n    \n    # Calculate the updates in a vectorized manner\n    update_value = epsilon * (padded_u[:-2, 1:-1] + padded_u[2:, 1:-1] + padded_u[1:-1, :-2] + padded_u[1:-1, 2:] - 4 * u)\n    u_new = u + update_value\n    \n    return u_new\n\n\n# JIT compile the function\n#advance_time_jax_jit = jit(advance_time_jax)"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "HW0\nIn this blog post, we will be creating a data visualization on the Palmer Penguins dataset.\n\nImporting libraries and reading data\nFirst, let’s import the necessary libararies and read the data into python.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\nAs seen in the code above, pandas is used to read the data. We will next use seaborn and matplotlib to create our plot.\n\n\nCreating the scatterplot\nNext, we will use the seaborn library to create our scatterplot. The scatterplot function takes the in the arguments data, x, y, and hue.\nsns.scatterplot(data=penguins, x=\"Culmen Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\")\n From this plot, we can see that there is a postive correlation between culmen length and body mass. Furthermore, we can see the differences between the 3 species of penguins.\nTo improve our visualization, let’s add a title and move the legend to the side of the plot.\nplt.legend(bbox_to_anchor=(1.05, 1),loc=2)\nplt.title(\"Scatter Plot of Culmen Length vs. Body Mass\")\n\n\n\noutput"
  },
  {
    "objectID": "posts/hw5/index.html",
    "href": "posts/hw5/index.html",
    "title": "HW5",
    "section": "",
    "text": "First we will import the necessary packages and obtain the data. We’ll use a sample data set from Kaggle that contains labeled images of cats and dogs.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import utils, layers, models\nimport tensorflow_datasets as tfds\nNext, we will create Datasets for training, validation, and testing. The dataset contains images of different sizes, so we resize them to a fixed size of 150x150.\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\nThe next block is technical code related to rapidly reading data. The batch_size determines how many data points are gathered from the directory at once.\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_dataset(dataset, num_samples=3, title=\"\"):\n    plt.figure(figsize=(15, 6))\n    # initialize empty sets for each animal\n    cat_images, dog_images = [], []\n    #for loop to iterate through each image in the dataset\n    for images, labels in dataset.take(1):\n      for image, label in zip(images, labels):\n            if label == 0:\n                cat_images.append(image.numpy())\n            else:\n                dog_images.append(image.numpy())\n    for i in range(num_samples):\n        # Plot cat images in the first row\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(cat_images[i].astype(\"uint8\"))\n        plt.title(\"Cat\")\n        plt.axis(\"off\")\n        # Plot dog images in the second row\n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(dog_images[i].astype(\"uint8\"))\n        plt.title(\"Dog\")\n        plt.axis(\"off\")\nplt.show()\n\nvisualize_dataset(train_ds, title=\"Random Samples from Training Dataset\")\n\n\n\noutput\n\n\n\n\n\nThe following line of code creates an iterator and computes the number of images in the training data with label 0 (corresponding to “cat”) and label 1 (corresponding to “dog”).\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n# create counters for cats and dogs\nnum_cats = 0\nnum_dogs = 0\n\n# loop the labels and count the number of cats and dogs\nfor label in labels_iterator:\n    if label == 0:\n        num_cats += 1\n    elif label == 1:\n        num_dogs += 1\n\n# show results\nprint(\"Number of cat images in the training data:\", num_cats)\nprint(\"Number of dog images in the training data:\", num_dogs)\nNumber of cat images in the training data: 4637\nNumber of dog images in the training data: 4668\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the number of dog images is greater than the number of cat images, we would expect the baseline model to always guess “dog”. Our models should do much better than the baseline.\n\n\n\nWe will use Conv2D layers, MaxPooling2D layers, a Flatten layer, a one Dense layer, and a one Dropout layer in this model.\nmodel1 = models.Sequential([\n    # Convolutional Layer 1\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    # Convolutional Layer 2\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flattening the output for the Dense layer\n    layers.Flatten(),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.5),\n\n    # Dense Layer\n    layers.Dense(64, activation='relu'),\n\n    # Output Layer\n    layers.Dense(10, activation='softmax')  \n])\nmodel1.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation)\nEpoch 1/20\n146/146 [==============================] - 6s 37ms/step - loss: 23.2708 - accuracy: 0.5278 - val_loss: 0.7293 - val_accuracy: 0.5735\nEpoch 2/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6757 - accuracy: 0.5940 - val_loss: 0.6561 - val_accuracy: 0.6152\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6433 - accuracy: 0.6403 - val_loss: 0.6458 - val_accuracy: 0.6208\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6024 - accuracy: 0.6677 - val_loss: 0.6974 - val_accuracy: 0.6217\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5541 - accuracy: 0.7083 - val_loss: 0.7306 - val_accuracy: 0.6255\nEpoch 6/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5206 - accuracy: 0.7394 - val_loss: 0.7694 - val_accuracy: 0.6195\nEpoch 7/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7892 - accuracy: 0.6905 - val_loss: 0.7744 - val_accuracy: 0.6152\nEpoch 8/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.5167 - accuracy: 0.7451 - val_loss: 0.7481 - val_accuracy: 0.6191\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4535 - accuracy: 0.7795 - val_loss: 0.8393 - val_accuracy: 0.6199\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3842 - accuracy: 0.8207 - val_loss: 0.8706 - val_accuracy: 0.6406\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3501 - accuracy: 0.8469 - val_loss: 0.9575 - val_accuracy: 0.6436\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3281 - accuracy: 0.8647 - val_loss: 1.3234 - val_accuracy: 0.6324\nEpoch 13/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3400 - accuracy: 0.8614 - val_loss: 1.0979 - val_accuracy: 0.6071\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3473 - accuracy: 0.8593 - val_loss: 0.9893 - val_accuracy: 0.6251\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2937 - accuracy: 0.8841 - val_loss: 1.1026 - val_accuracy: 0.6303\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2500 - accuracy: 0.8976 - val_loss: 0.9978 - val_accuracy: 0.6513\nEpoch 17/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2418 - accuracy: 0.9080 - val_loss: 1.0302 - val_accuracy: 0.6260\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2503 - accuracy: 0.9012 - val_loss: 1.0148 - val_accuracy: 0.6333\nEpoch 19/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1845 - accuracy: 0.9303 - val_loss: 1.0552 - val_accuracy: 0.6440\nEpoch 20/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1561 - accuracy: 0.9463 - val_loss: 1.1136 - val_accuracy: 0.6406\n# plotting validation and training accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n The model stabilized at at accuracy of around 62% during training. There does to appear to be overfitting, as the training accuracy tends to be about became significantly higher than the validation accuracy as the amount of epochs increased.\n\n\n\nLet’s try to improve upon our first model by augmenting the amount of data that is supplied to train the model. Two ways that we can augment data for images would be by rotating or flipping the image. This would allow the model to find invariant features of the input image. Let’s try out some functions to flip and rotate the images below.\nplt.imshow(image[0] / 255.0)\nplt.show()\n\n# try one flip\nflip1 = layers.RandomFlip()(image[0] / 255.0)\nplt.imshow(flip1)\nplt.show()\n\n# try another flip\nflip2 = layers.RandomFlip()(image[0] / 255.0)\nplt.imshow(flip2)\nplt.show()\n  \nplt.imshow(image[0]/ 255.0)\nplt.show()\n\n# try one rotation\nrot1 = layers.RandomRotation(90)(image[0] / 255.0)\nplt.imshow(rot1)\nplt.show()\n\n# try another rotation\nrot2 = layers.RandomRotation(180)(image[0] / 255.0)\nplt.imshow(rot2)\nplt.show()\n  \nNow that we understand how RandomFlip and RandomRotation work, we can add them as layers to our model.\nmodel2 = models.Sequential([\n    layers.RandomFlip(mode=\"horizontal\"),\n    layers.RandomRotation(30),\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 401s 3s/step - loss: 17.6323 - accuracy: 0.5212 - val_loss: 0.6956 - val_accuracy: 0.5365\nEpoch 2/20\n146/146 [==============================] - 403s 3s/step - loss: 0.7176 - accuracy: 0.5573 - val_loss: 0.6806 - val_accuracy: 0.5589\nEpoch 3/20\n146/146 [==============================] - 417s 3s/step - loss: 0.6885 - accuracy: 0.5818 - val_loss: 0.6533 - val_accuracy: 0.6053\nEpoch 4/20\n146/146 [==============================] - 408s 3s/step - loss: 0.6953 - accuracy: 0.5904 - val_loss: 0.6744 - val_accuracy: 0.6006\nEpoch 5/20\n146/146 [==============================] - 396s 3s/step - loss: 0.7086 - accuracy: 0.5843 - val_loss: 0.6449 - val_accuracy: 0.6221\nEpoch 6/20\n146/146 [==============================] - 404s 3s/step - loss: 0.6983 - accuracy: 0.5887 - val_loss: 0.6296 - val_accuracy: 0.6225\nEpoch 7/20\n146/146 [==============================] - 381s 3s/step - loss: 0.6820 - accuracy: 0.5929 - val_loss: 0.6643 - val_accuracy: 0.6088\nEpoch 8/20\n146/146 [==============================] - 389s 3s/step - loss: 0.6767 - accuracy: 0.6048 - val_loss: 0.6355 - val_accuracy: 0.6264\nEpoch 9/20\n146/146 [==============================] - 406s 3s/step - loss: 0.7055 - accuracy: 0.5922 - val_loss: 0.6611 - val_accuracy: 0.6225\nEpoch 10/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6675 - accuracy: 0.6046 - val_loss: 0.6416 - val_accuracy: 0.6307\nEpoch 11/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6710 - accuracy: 0.6043 - val_loss: 0.6314 - val_accuracy: 0.6376\nEpoch 12/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6570 - accuracy: 0.6160 - val_loss: 0.6486 - val_accuracy: 0.6324\nEpoch 13/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6671 - accuracy: 0.6143 - val_loss: 0.6633 - val_accuracy: 0.6255\nEpoch 14/20\n146/146 [==============================] - 403s 3s/step - loss: 0.6822 - accuracy: 0.6008 - val_loss: 0.6647 - val_accuracy: 0.6109\nEpoch 15/20\n146/146 [==============================] - 384s 3s/step - loss: 0.6487 - accuracy: 0.6152 - val_loss: 0.6157 - val_accuracy: 0.6423\nEpoch 16/20\n146/146 [==============================] - 397s 3s/step - loss: 0.6517 - accuracy: 0.6188 - val_loss: 0.6323 - val_accuracy: 0.6367\nEpoch 17/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6457 - accuracy: 0.6244 - val_loss: 0.6372 - val_accuracy: 0.6414\nEpoch 18/20\n146/146 [==============================] - 400s 3s/step - loss: 0.6498 - accuracy: 0.6289 - val_loss: 0.6078 - val_accuracy: 0.6531\nEpoch 19/20\n146/146 [==============================] - 407s 3s/step - loss: 0.6616 - accuracy: 0.6015 - val_loss: 0.7009 - val_accuracy: 0.5460\nEpoch 20/20\n146/146 [==============================] - 401s 3s/step - loss: 0.6945 - accuracy: 0.5402 - val_loss: 0.6512 - val_accuracy: 0.5924\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\noutput\n\n\nThe validation accuracy stabilized around 58-62% during training. The model does not appear to be overfit, as the training and validation accuracies are relatively similar.\n\n\n\nTo further improve our model, we can also add a preprocessing step. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. Let’s see how preprocessing changes performance.\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(factor=0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 9s 46ms/step - loss: 0.6913 - accuracy: 0.5736 - val_loss: 0.6139 - val_accuracy: 0.6522\nEpoch 2/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6085 - accuracy: 0.6651 - val_loss: 0.5345 - val_accuracy: 0.7283\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5635 - accuracy: 0.7074 - val_loss: 0.5110 - val_accuracy: 0.7549\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5410 - accuracy: 0.7279 - val_loss: 0.4855 - val_accuracy: 0.7760\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5156 - accuracy: 0.7444 - val_loss: 0.4747 - val_accuracy: 0.7739\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5095 - accuracy: 0.7492 - val_loss: 0.4503 - val_accuracy: 0.7919\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4917 - accuracy: 0.7617 - val_loss: 0.4603 - val_accuracy: 0.7837\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4800 - accuracy: 0.7640 - val_loss: 0.4310 - val_accuracy: 0.7928\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4704 - accuracy: 0.7789 - val_loss: 0.4271 - val_accuracy: 0.8052\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4586 - accuracy: 0.7851 - val_loss: 0.4298 - val_accuracy: 0.8087\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4461 - accuracy: 0.7920 - val_loss: 0.4004 - val_accuracy: 0.8160\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4304 - accuracy: 0.8012 - val_loss: 0.4148 - val_accuracy: 0.8160\nEpoch 13/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4356 - accuracy: 0.7981 - val_loss: 0.3971 - val_accuracy: 0.8220\nEpoch 14/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4151 - accuracy: 0.8109 - val_loss: 0.3968 - val_accuracy: 0.8289\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4123 - accuracy: 0.8134 - val_loss: 0.3818 - val_accuracy: 0.8328\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4042 - accuracy: 0.8204 - val_loss: 0.3885 - val_accuracy: 0.8276\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3926 - accuracy: 0.8205 - val_loss: 0.3910 - val_accuracy: 0.8280\nEpoch 18/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3810 - accuracy: 0.8336 - val_loss: 0.3956 - val_accuracy: 0.8336\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3790 - accuracy: 0.8271 - val_loss: 0.4081 - val_accuracy: 0.8319\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3683 - accuracy: 0.8347 - val_loss: 0.3920 - val_accuracy: 0.8366\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n The validation accuracy stabilized around 80-82% during training. This is a significant improvement from what was obtained in model 1. The model also does not appear to be overfit, as the training and validation accuracies are similar.\n\n\n\nIn model 4, we wil use MobileNetV2, a model which has been trained for image classification, as a base layer for our model. We can use the code below to download MobileNetV2 and configure it as a layer.\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\nNow, we can create our model. We will still use our data augmentation techniques in the previous models in addition to the pre-trained base layer that we downloaded.\nmodel4 = models.Sequential([\n      layers.RandomFlip(\"horizontal\"),\n      layers.RandomRotation(180),\n      base_model_layer,\n      layers.GlobalAveragePooling2D(),\n      layers.Dropout(0.2),\n      layers.Dense(2)\n])\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel4.summary()\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_6 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_6 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_3 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_average_pooling2d_4  (None, 960)               0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dropout_6 (Dropout)         (None, 960)               0         \n                                                                 \n dense_8 (Dense)             (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 14s 49ms/step - loss: 0.9394 - accuracy: 0.6898 - val_loss: 0.7074 - val_accuracy: 0.9089\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.7334 - accuracy: 0.8441 - val_loss: 0.6821 - val_accuracy: 0.9239\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6824 - accuracy: 0.8476 - val_loss: 0.6863 - val_accuracy: 0.9273\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6924 - accuracy: 0.8404 - val_loss: 0.6884 - val_accuracy: 0.9192\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6872 - accuracy: 0.8443 - val_loss: 0.6872 - val_accuracy: 0.9209\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6887 - accuracy: 0.8404 - val_loss: 0.6887 - val_accuracy: 0.9222\nEpoch 7/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6877 - accuracy: 0.8376 - val_loss: 0.6890 - val_accuracy: 0.9200\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6861 - accuracy: 0.8334 - val_loss: 0.6890 - val_accuracy: 0.9200\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6903 - accuracy: 0.8437 - val_loss: 0.6893 - val_accuracy: 0.9239\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6898 - accuracy: 0.8420 - val_loss: 0.6893 - val_accuracy: 0.9239\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6910 - accuracy: 0.8465 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6911 - accuracy: 0.8502 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6911 - accuracy: 0.8506 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 14/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6909 - accuracy: 0.8478 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6913 - accuracy: 0.8493 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6914 - accuracy: 0.8477 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6906 - accuracy: 0.8469 - val_loss: 0.6899 - val_accuracy: 0.9273\nEpoch 18/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.6937 - accuracy: 0.8447 - val_loss: 0.6931 - val_accuracy: 0.9278\nEpoch 19/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6921 - accuracy: 0.8421 - val_loss: 0.6931 - val_accuracy: 0.9273\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6925 - accuracy: 0.8409 - val_loss: 0.6931 - val_accuracy: 0.9273\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\noutput\n\n\nThe validation accuracy stabilized around 93% during training. This is a large improvement from what was obtained in model 1. The model also does not appear to be overfit, as the training accuracy is actually lower than the validation accuracy for all of the epochs.\n\n\n\nLet’s see how model4 performs on unseen test data.\n# Score on test data\nloss, acc = model4.evaluate(test_ds)\nprint('Test accuracy:', acc)\n37/37 [==============================] - 3s 75ms/step - loss: 0.6926 - accuracy: 0.9351\nTest accuracy: 0.9350816607475281\nThe test accuracy is very high at 93.5%, meaning that our model performs well!"
  },
  {
    "objectID": "posts/hw5/index.html#load-packages-and-obtain-data",
    "href": "posts/hw5/index.html#load-packages-and-obtain-data",
    "title": "HW5",
    "section": "",
    "text": "First we will import the necessary packages and obtain the data. We’ll use a sample data set from Kaggle that contains labeled images of cats and dogs.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import utils, layers, models\nimport tensorflow_datasets as tfds\nNext, we will create Datasets for training, validation, and testing. The dataset contains images of different sizes, so we resize them to a fixed size of 150x150.\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\nThe next block is technical code related to rapidly reading data. The batch_size determines how many data points are gathered from the directory at once.\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/hw5/index.html#visualizing-the-dataset",
    "href": "posts/hw5/index.html#visualizing-the-dataset",
    "title": "HW5",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_dataset(dataset, num_samples=3, title=\"\"):\n    plt.figure(figsize=(15, 6))\n    # initialize empty sets for each animal\n    cat_images, dog_images = [], []\n    #for loop to iterate through each image in the dataset\n    for images, labels in dataset.take(1):\n      for image, label in zip(images, labels):\n            if label == 0:\n                cat_images.append(image.numpy())\n            else:\n                dog_images.append(image.numpy())\n    for i in range(num_samples):\n        # Plot cat images in the first row\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(cat_images[i].astype(\"uint8\"))\n        plt.title(\"Cat\")\n        plt.axis(\"off\")\n        # Plot dog images in the second row\n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(dog_images[i].astype(\"uint8\"))\n        plt.title(\"Dog\")\n        plt.axis(\"off\")\nplt.show()\n\nvisualize_dataset(train_ds, title=\"Random Samples from Training Dataset\")\n\n\n\noutput"
  },
  {
    "objectID": "posts/hw5/index.html#checking-label-frequencies",
    "href": "posts/hw5/index.html#checking-label-frequencies",
    "title": "HW5",
    "section": "",
    "text": "The following line of code creates an iterator and computes the number of images in the training data with label 0 (corresponding to “cat”) and label 1 (corresponding to “dog”).\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n# create counters for cats and dogs\nnum_cats = 0\nnum_dogs = 0\n\n# loop the labels and count the number of cats and dogs\nfor label in labels_iterator:\n    if label == 0:\n        num_cats += 1\n    elif label == 1:\n        num_dogs += 1\n\n# show results\nprint(\"Number of cat images in the training data:\", num_cats)\nprint(\"Number of dog images in the training data:\", num_dogs)\nNumber of cat images in the training data: 4637\nNumber of dog images in the training data: 4668\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the number of dog images is greater than the number of cat images, we would expect the baseline model to always guess “dog”. Our models should do much better than the baseline."
  },
  {
    "objectID": "posts/hw5/index.html#first-model",
    "href": "posts/hw5/index.html#first-model",
    "title": "HW5",
    "section": "",
    "text": "We will use Conv2D layers, MaxPooling2D layers, a Flatten layer, a one Dense layer, and a one Dropout layer in this model.\nmodel1 = models.Sequential([\n    # Convolutional Layer 1\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    # Convolutional Layer 2\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flattening the output for the Dense layer\n    layers.Flatten(),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.5),\n\n    # Dense Layer\n    layers.Dense(64, activation='relu'),\n\n    # Output Layer\n    layers.Dense(10, activation='softmax')  \n])\nmodel1.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation)\nEpoch 1/20\n146/146 [==============================] - 6s 37ms/step - loss: 23.2708 - accuracy: 0.5278 - val_loss: 0.7293 - val_accuracy: 0.5735\nEpoch 2/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6757 - accuracy: 0.5940 - val_loss: 0.6561 - val_accuracy: 0.6152\nEpoch 3/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6433 - accuracy: 0.6403 - val_loss: 0.6458 - val_accuracy: 0.6208\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6024 - accuracy: 0.6677 - val_loss: 0.6974 - val_accuracy: 0.6217\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.5541 - accuracy: 0.7083 - val_loss: 0.7306 - val_accuracy: 0.6255\nEpoch 6/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.5206 - accuracy: 0.7394 - val_loss: 0.7694 - val_accuracy: 0.6195\nEpoch 7/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7892 - accuracy: 0.6905 - val_loss: 0.7744 - val_accuracy: 0.6152\nEpoch 8/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.5167 - accuracy: 0.7451 - val_loss: 0.7481 - val_accuracy: 0.6191\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4535 - accuracy: 0.7795 - val_loss: 0.8393 - val_accuracy: 0.6199\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3842 - accuracy: 0.8207 - val_loss: 0.8706 - val_accuracy: 0.6406\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3501 - accuracy: 0.8469 - val_loss: 0.9575 - val_accuracy: 0.6436\nEpoch 12/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3281 - accuracy: 0.8647 - val_loss: 1.3234 - val_accuracy: 0.6324\nEpoch 13/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3400 - accuracy: 0.8614 - val_loss: 1.0979 - val_accuracy: 0.6071\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3473 - accuracy: 0.8593 - val_loss: 0.9893 - val_accuracy: 0.6251\nEpoch 15/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2937 - accuracy: 0.8841 - val_loss: 1.1026 - val_accuracy: 0.6303\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2500 - accuracy: 0.8976 - val_loss: 0.9978 - val_accuracy: 0.6513\nEpoch 17/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.2418 - accuracy: 0.9080 - val_loss: 1.0302 - val_accuracy: 0.6260\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2503 - accuracy: 0.9012 - val_loss: 1.0148 - val_accuracy: 0.6333\nEpoch 19/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1845 - accuracy: 0.9303 - val_loss: 1.0552 - val_accuracy: 0.6440\nEpoch 20/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.1561 - accuracy: 0.9463 - val_loss: 1.1136 - val_accuracy: 0.6406\n# plotting validation and training accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n The model stabilized at at accuracy of around 62% during training. There does to appear to be overfitting, as the training accuracy tends to be about became significantly higher than the validation accuracy as the amount of epochs increased."
  },
  {
    "objectID": "posts/hw5/index.html#data-augmentation",
    "href": "posts/hw5/index.html#data-augmentation",
    "title": "HW5",
    "section": "",
    "text": "Let’s try to improve upon our first model by augmenting the amount of data that is supplied to train the model. Two ways that we can augment data for images would be by rotating or flipping the image. This would allow the model to find invariant features of the input image. Let’s try out some functions to flip and rotate the images below.\nplt.imshow(image[0] / 255.0)\nplt.show()\n\n# try one flip\nflip1 = layers.RandomFlip()(image[0] / 255.0)\nplt.imshow(flip1)\nplt.show()\n\n# try another flip\nflip2 = layers.RandomFlip()(image[0] / 255.0)\nplt.imshow(flip2)\nplt.show()\n  \nplt.imshow(image[0]/ 255.0)\nplt.show()\n\n# try one rotation\nrot1 = layers.RandomRotation(90)(image[0] / 255.0)\nplt.imshow(rot1)\nplt.show()\n\n# try another rotation\nrot2 = layers.RandomRotation(180)(image[0] / 255.0)\nplt.imshow(rot2)\nplt.show()\n  \nNow that we understand how RandomFlip and RandomRotation work, we can add them as layers to our model.\nmodel2 = models.Sequential([\n    layers.RandomFlip(mode=\"horizontal\"),\n    layers.RandomRotation(30),\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 401s 3s/step - loss: 17.6323 - accuracy: 0.5212 - val_loss: 0.6956 - val_accuracy: 0.5365\nEpoch 2/20\n146/146 [==============================] - 403s 3s/step - loss: 0.7176 - accuracy: 0.5573 - val_loss: 0.6806 - val_accuracy: 0.5589\nEpoch 3/20\n146/146 [==============================] - 417s 3s/step - loss: 0.6885 - accuracy: 0.5818 - val_loss: 0.6533 - val_accuracy: 0.6053\nEpoch 4/20\n146/146 [==============================] - 408s 3s/step - loss: 0.6953 - accuracy: 0.5904 - val_loss: 0.6744 - val_accuracy: 0.6006\nEpoch 5/20\n146/146 [==============================] - 396s 3s/step - loss: 0.7086 - accuracy: 0.5843 - val_loss: 0.6449 - val_accuracy: 0.6221\nEpoch 6/20\n146/146 [==============================] - 404s 3s/step - loss: 0.6983 - accuracy: 0.5887 - val_loss: 0.6296 - val_accuracy: 0.6225\nEpoch 7/20\n146/146 [==============================] - 381s 3s/step - loss: 0.6820 - accuracy: 0.5929 - val_loss: 0.6643 - val_accuracy: 0.6088\nEpoch 8/20\n146/146 [==============================] - 389s 3s/step - loss: 0.6767 - accuracy: 0.6048 - val_loss: 0.6355 - val_accuracy: 0.6264\nEpoch 9/20\n146/146 [==============================] - 406s 3s/step - loss: 0.7055 - accuracy: 0.5922 - val_loss: 0.6611 - val_accuracy: 0.6225\nEpoch 10/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6675 - accuracy: 0.6046 - val_loss: 0.6416 - val_accuracy: 0.6307\nEpoch 11/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6710 - accuracy: 0.6043 - val_loss: 0.6314 - val_accuracy: 0.6376\nEpoch 12/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6570 - accuracy: 0.6160 - val_loss: 0.6486 - val_accuracy: 0.6324\nEpoch 13/20\n146/146 [==============================] - 387s 3s/step - loss: 0.6671 - accuracy: 0.6143 - val_loss: 0.6633 - val_accuracy: 0.6255\nEpoch 14/20\n146/146 [==============================] - 403s 3s/step - loss: 0.6822 - accuracy: 0.6008 - val_loss: 0.6647 - val_accuracy: 0.6109\nEpoch 15/20\n146/146 [==============================] - 384s 3s/step - loss: 0.6487 - accuracy: 0.6152 - val_loss: 0.6157 - val_accuracy: 0.6423\nEpoch 16/20\n146/146 [==============================] - 397s 3s/step - loss: 0.6517 - accuracy: 0.6188 - val_loss: 0.6323 - val_accuracy: 0.6367\nEpoch 17/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6457 - accuracy: 0.6244 - val_loss: 0.6372 - val_accuracy: 0.6414\nEpoch 18/20\n146/146 [==============================] - 400s 3s/step - loss: 0.6498 - accuracy: 0.6289 - val_loss: 0.6078 - val_accuracy: 0.6531\nEpoch 19/20\n146/146 [==============================] - 407s 3s/step - loss: 0.6616 - accuracy: 0.6015 - val_loss: 0.7009 - val_accuracy: 0.5460\nEpoch 20/20\n146/146 [==============================] - 401s 3s/step - loss: 0.6945 - accuracy: 0.5402 - val_loss: 0.6512 - val_accuracy: 0.5924\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\noutput\n\n\nThe validation accuracy stabilized around 58-62% during training. The model does not appear to be overfit, as the training and validation accuracies are relatively similar."
  },
  {
    "objectID": "posts/hw5/index.html#data-preprocessing",
    "href": "posts/hw5/index.html#data-preprocessing",
    "title": "HW5",
    "section": "",
    "text": "To further improve our model, we can also add a preprocessing step. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. Let’s see how preprocessing changes performance.\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(factor=0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 9s 46ms/step - loss: 0.6913 - accuracy: 0.5736 - val_loss: 0.6139 - val_accuracy: 0.6522\nEpoch 2/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6085 - accuracy: 0.6651 - val_loss: 0.5345 - val_accuracy: 0.7283\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5635 - accuracy: 0.7074 - val_loss: 0.5110 - val_accuracy: 0.7549\nEpoch 4/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5410 - accuracy: 0.7279 - val_loss: 0.4855 - val_accuracy: 0.7760\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5156 - accuracy: 0.7444 - val_loss: 0.4747 - val_accuracy: 0.7739\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5095 - accuracy: 0.7492 - val_loss: 0.4503 - val_accuracy: 0.7919\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4917 - accuracy: 0.7617 - val_loss: 0.4603 - val_accuracy: 0.7837\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4800 - accuracy: 0.7640 - val_loss: 0.4310 - val_accuracy: 0.7928\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4704 - accuracy: 0.7789 - val_loss: 0.4271 - val_accuracy: 0.8052\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4586 - accuracy: 0.7851 - val_loss: 0.4298 - val_accuracy: 0.8087\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4461 - accuracy: 0.7920 - val_loss: 0.4004 - val_accuracy: 0.8160\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4304 - accuracy: 0.8012 - val_loss: 0.4148 - val_accuracy: 0.8160\nEpoch 13/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4356 - accuracy: 0.7981 - val_loss: 0.3971 - val_accuracy: 0.8220\nEpoch 14/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4151 - accuracy: 0.8109 - val_loss: 0.3968 - val_accuracy: 0.8289\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4123 - accuracy: 0.8134 - val_loss: 0.3818 - val_accuracy: 0.8328\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4042 - accuracy: 0.8204 - val_loss: 0.3885 - val_accuracy: 0.8276\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.3926 - accuracy: 0.8205 - val_loss: 0.3910 - val_accuracy: 0.8280\nEpoch 18/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3810 - accuracy: 0.8336 - val_loss: 0.3956 - val_accuracy: 0.8336\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3790 - accuracy: 0.8271 - val_loss: 0.4081 - val_accuracy: 0.8319\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3683 - accuracy: 0.8347 - val_loss: 0.3920 - val_accuracy: 0.8366\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n The validation accuracy stabilized around 80-82% during training. This is a significant improvement from what was obtained in model 1. The model also does not appear to be overfit, as the training and validation accuracies are similar."
  },
  {
    "objectID": "posts/hw5/index.html#transfer-learning",
    "href": "posts/hw5/index.html#transfer-learning",
    "title": "HW5",
    "section": "",
    "text": "In model 4, we wil use MobileNetV2, a model which has been trained for image classification, as a base layer for our model. We can use the code below to download MobileNetV2 and configure it as a layer.\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\nNow, we can create our model. We will still use our data augmentation techniques in the previous models in addition to the pre-trained base layer that we downloaded.\nmodel4 = models.Sequential([\n      layers.RandomFlip(\"horizontal\"),\n      layers.RandomRotation(180),\n      base_model_layer,\n      layers.GlobalAveragePooling2D(),\n      layers.Dropout(0.2),\n      layers.Dense(2)\n])\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel4.summary()\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_6 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_6 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_3 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_average_pooling2d_4  (None, 960)               0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dropout_6 (Dropout)         (None, 960)               0         \n                                                                 \n dense_8 (Dense)             (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\nEpoch 1/20\n146/146 [==============================] - 14s 49ms/step - loss: 0.9394 - accuracy: 0.6898 - val_loss: 0.7074 - val_accuracy: 0.9089\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.7334 - accuracy: 0.8441 - val_loss: 0.6821 - val_accuracy: 0.9239\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6824 - accuracy: 0.8476 - val_loss: 0.6863 - val_accuracy: 0.9273\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6924 - accuracy: 0.8404 - val_loss: 0.6884 - val_accuracy: 0.9192\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6872 - accuracy: 0.8443 - val_loss: 0.6872 - val_accuracy: 0.9209\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6887 - accuracy: 0.8404 - val_loss: 0.6887 - val_accuracy: 0.9222\nEpoch 7/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6877 - accuracy: 0.8376 - val_loss: 0.6890 - val_accuracy: 0.9200\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6861 - accuracy: 0.8334 - val_loss: 0.6890 - val_accuracy: 0.9200\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6903 - accuracy: 0.8437 - val_loss: 0.6893 - val_accuracy: 0.9239\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6898 - accuracy: 0.8420 - val_loss: 0.6893 - val_accuracy: 0.9239\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6910 - accuracy: 0.8465 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6911 - accuracy: 0.8502 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6911 - accuracy: 0.8506 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 14/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6909 - accuracy: 0.8478 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6913 - accuracy: 0.8493 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6914 - accuracy: 0.8477 - val_loss: 0.6908 - val_accuracy: 0.9273\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6906 - accuracy: 0.8469 - val_loss: 0.6899 - val_accuracy: 0.9273\nEpoch 18/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.6937 - accuracy: 0.8447 - val_loss: 0.6931 - val_accuracy: 0.9278\nEpoch 19/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.6921 - accuracy: 0.8421 - val_loss: 0.6931 - val_accuracy: 0.9273\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6925 - accuracy: 0.8409 - val_loss: 0.6931 - val_accuracy: 0.9273\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\noutput\n\n\nThe validation accuracy stabilized around 93% during training. This is a large improvement from what was obtained in model 1. The model also does not appear to be overfit, as the training accuracy is actually lower than the validation accuracy for all of the epochs."
  },
  {
    "objectID": "posts/hw5/index.html#score-on-test-data",
    "href": "posts/hw5/index.html#score-on-test-data",
    "title": "HW5",
    "section": "",
    "text": "Let’s see how model4 performs on unseen test data.\n# Score on test data\nloss, acc = model4.evaluate(test_ds)\nprint('Test accuracy:', acc)\n37/37 [==============================] - 3s 75ms/step - loss: 0.6926 - accuracy: 0.9351\nTest accuracy: 0.9350816607475281\nThe test accuracy is very high at 93.5%, meaning that our model performs well!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "HW3",
    "section": "",
    "text": "First, let’s import the necessary libraries.\nfrom flask import Flask, g, render_template, request\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\nimport sqlite3\nimport sys\n\napp = Flask(__name__)\nNext, let’s define the get_message_db() function.\ndef get_message_db():\n    try:\n        return g.message_db\n    except:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY,\n            handle TEXT,\n            message TEXT\n        )\n        '''\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\nThe purpose of this is to ensure that there is a connection to a SQLite database where messages can be stored. It either returns an existing connection stored in the Flask global object (g.message_db) or creates a new connection if one does not exist. After creating a new connection, the function executes a SQL command to create a table named messages if it doesn’t already exist. The table has three columns: id, handle, and message.\nThe next function is the insert_message() function.\ndef insert_message(request):\n    message = request.form[\"message\"]\n    handle = request.form[\"handle\"]\n\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    conn.commit()\n    cursor.close()\n\n    return message, handle\nThis function takes in a request object and returns the message and handle that were inserted into the database. The overall purpose of this function is to insert messages into the database.\nNext, we can define the random_messages() function.\ndef random_messages(n):\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    return messages\nThis function takes an integer n as its argument, indicating the number of random messages to retrieve from the database. After obtaining the connection, the function creates a cursor object to execute SQL commands. This function is useful for displaying random messages to users.\nThe last function we define is the render_submit_template().\ndef render_submit_template():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    elif request.method == 'POST':\n        message, handle = insert_message(request)\n        return render_template('submit.html')\nThe purpose of this function is to handle rendering of the submit.html template for both initial GET requests (to serve the form) and POST requests (after form submission). If the request method is ‘POST’, insert_message(request) is called.\nFinally, let’s define our app routes.\n@app.route('/')\ndef home():\n    return render_template('base.html')\n\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    return render_submit_template()\n\n@app.route('/view/')\ndef view():\n    messages = random_messages(5)  # Get 5 random messages\n    return render_template('view.html', messages=messages)\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))\n\n\n\nsubmit.html is one of the template files used in the app.\n{% extends \"base.html\" %} \n\n{% block content %}\n    &lt;form method=\"post\" enctype=\"multipart/form-data\"&gt;\n        &lt;label for=\"message\"&gt;Message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"message\" name=\"message\"&gt;&lt;br&gt;\n        &lt;label for=\"name\"&gt;Your Name:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"handle\" name=\"handle\"&gt;&lt;br&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit form\"&gt;\n    &lt;/form&gt;\n{% endblock %}\nThe first line indicates that this template extends another template named base.html and inherits the structure and layout. The content block creates fields from user input, and defines an HTML form with fields for a message and the user’s name.\n\n\n\nThis screencap shows an example of a user submitting a message. In this case, the message is “Hello” while the handle is “Jessica Xiao”. \nThis screencap shows an example of a user viewing submitted messages. When the user clicks “view”, they will be able to view 5 randomly generated messages from the database."
  },
  {
    "objectID": "posts/hw3/index.html#write-the-app.py-file",
    "href": "posts/hw3/index.html#write-the-app.py-file",
    "title": "HW3",
    "section": "",
    "text": "First, let’s import the necessary libraries.\nfrom flask import Flask, g, render_template, request\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\nimport sqlite3\nimport sys\n\napp = Flask(__name__)\nNext, let’s define the get_message_db() function.\ndef get_message_db():\n    try:\n        return g.message_db\n    except:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            id INTEGER PRIMARY KEY,\n            handle TEXT,\n            message TEXT\n        )\n        '''\n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\nThe purpose of this is to ensure that there is a connection to a SQLite database where messages can be stored. It either returns an existing connection stored in the Flask global object (g.message_db) or creates a new connection if one does not exist. After creating a new connection, the function executes a SQL command to create a table named messages if it doesn’t already exist. The table has three columns: id, handle, and message.\nThe next function is the insert_message() function.\ndef insert_message(request):\n    message = request.form[\"message\"]\n    handle = request.form[\"handle\"]\n\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    conn.commit()\n    cursor.close()\n\n    return message, handle\nThis function takes in a request object and returns the message and handle that were inserted into the database. The overall purpose of this function is to insert messages into the database.\nNext, we can define the random_messages() function.\ndef random_messages(n):\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    return messages\nThis function takes an integer n as its argument, indicating the number of random messages to retrieve from the database. After obtaining the connection, the function creates a cursor object to execute SQL commands. This function is useful for displaying random messages to users.\nThe last function we define is the render_submit_template().\ndef render_submit_template():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    elif request.method == 'POST':\n        message, handle = insert_message(request)\n        return render_template('submit.html')\nThe purpose of this function is to handle rendering of the submit.html template for both initial GET requests (to serve the form) and POST requests (after form submission). If the request method is ‘POST’, insert_message(request) is called.\nFinally, let’s define our app routes.\n@app.route('/')\ndef home():\n    return render_template('base.html')\n\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    return render_submit_template()\n\n@app.route('/view/')\ndef view():\n    messages = random_messages(5)  # Get 5 random messages\n    return render_template('view.html', messages=messages)\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))"
  },
  {
    "objectID": "posts/hw3/index.html#explanation-of-base.html",
    "href": "posts/hw3/index.html#explanation-of-base.html",
    "title": "HW3",
    "section": "",
    "text": "submit.html is one of the template files used in the app.\n{% extends \"base.html\" %} \n\n{% block content %}\n    &lt;form method=\"post\" enctype=\"multipart/form-data\"&gt;\n        &lt;label for=\"message\"&gt;Message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"message\" name=\"message\"&gt;&lt;br&gt;\n        &lt;label for=\"name\"&gt;Your Name:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" id=\"handle\" name=\"handle\"&gt;&lt;br&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit form\"&gt;\n    &lt;/form&gt;\n{% endblock %}\nThe first line indicates that this template extends another template named base.html and inherits the structure and layout. The content block creates fields from user input, and defines an HTML form with fields for a message and the user’s name."
  },
  {
    "objectID": "posts/hw3/index.html#examples-of-user-submissions",
    "href": "posts/hw3/index.html#examples-of-user-submissions",
    "title": "HW3",
    "section": "",
    "text": "This screencap shows an example of a user submitting a message. In this case, the message is “Hello” while the handle is “Jessica Xiao”. \nThis screencap shows an example of a user viewing submitted messages. When the user clicks “view”, they will be able to view 5 randomly generated messages from the database."
  },
  {
    "objectID": "posts/hw6/index.html",
    "href": "posts/hw6/index.html",
    "title": "HW6",
    "section": "",
    "text": "In this homework, we are going to create a fake news classifier with Keras.\nFirst, we will import the necessary libraries.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport tensorflow as tf\nimport re\nimport string\nimport keras\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\n\nThe data that we will use to train the model can be found at the link below. We will read the data into a pandas dataframe.\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\ntrain_data = pd.read_csv(train_url)\ntrain_data\n We see that this dataset contains information about 22449 articles. Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.\n\n\n\nNext, let’s write the function make_dataset to convert the data to a tensorflow dataset.\nbatch_size = 100\ndef make_dataset(df):\n    # Load stopwords from NLTK\n    stop_words = stopwords.words('english')\n    \n    # remove stopwords\n    df['title'] = df['title'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    df['text'] = df['text'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    \n    # Construct a tf.data.Dataset\n    # Note: Ensure your DataFrame has columns named 'title', 'text', and 'fake' for this to work directly\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                \"title\": df['title'].values,  # Pass series.values to get numpy representation\n                \"text\": df['text'].values\n            },\n            df['fake'].values\n        )\n    )\n    \n    # Batch the dataset\n    dataset = dataset.batch(batch_size)\n    \n    return dataset\n\ndata = make_dataset(train_data)\nNow let’s split our data into training and validation sets. We will use 20% for validation. Again, we will batch the data so it will train more efficiently.\ndata = data.shuffle(buffer_size = len(data), reshuffle_each_iteration=False)\ntrain_size = int(0.8 * len(data)) # allocate 80% of data for training\n\ntrain = data.take(train_size) # set training data\nval = data.skip(train_size) # set validation data\n# check the size of training, validation, testing set\nlen(train), len(val)\n(180, 45)\nHere we see that the training data is four times larger than the validation data, which is what we expect.\nclass_counts = train_data['fake'].value_counts(normalize=True)\n\n# The base rate will be the proportion of the most common class\nbase_rate = class_counts.max()\nbase_rate\n0.522963160942581\n\n\n\nIn the code below, we standardize the the data by turning all text to lowercase and removing punctuation. We also vectorize the text through creating a text vectorization layer.\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n# Define the shared embedding layer\nembedding_layer = layers.Embedding(size_vocabulary, 20, name = \"embedding\")\n\n# Define the input layers\ntitle_input = keras.Input(\n    shape = (1,), \n    # name for us to remember for later\n    name = \"title\",\n    # type of data contained\n    dtype = \"string\"\n)\n\ntext_input = keras.Input(\n    shape = (1,), \n    name = \"text\",\n    dtype = \"string\"\n)\n\n\n\n\n\nThe first model will determine whether an article contains fake news or not solely based on its title. To do so, we will use the functional API of TensorFlow. We will define a pipeline of hidden layers to process the titles. First, we explicitly define an embedding layer. This is so we can reuse it in a later model. Then we define different layers for the text data. The dropout layers prevent overfitting.\n# Model 1: Only article title as input\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = embedding_layer(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntitle_output = layers.Dense(2,name='fake')(title_features)\nmodel_title = keras.Model(\n    inputs = title_input,\n    outputs = title_output\n)\nmodel_title.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory_title = model_title.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - accuracy: 0.5163 - loss: 0.6932 - val_accuracy: 0.5289 - val_loss: 0.6891\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5299 - loss: 0.6899 - val_accuracy: 0.5289 - val_loss: 0.6836\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5696 - loss: 0.6799 - val_accuracy: 0.7111 - val_loss: 0.6536\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6596 - loss: 0.6401 - val_accuracy: 0.7742 - val_loss: 0.5562\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7271 - loss: 0.5563 - val_accuracy: 0.7924 - val_loss: 0.4796\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7548 - loss: 0.4996 - val_accuracy: 0.8087 - val_loss: 0.4375\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7870 - loss: 0.4572 - val_accuracy: 0.8176 - val_loss: 0.4127\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8050 - loss: 0.4236 - val_accuracy: 0.8273 - val_loss: 0.3870\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8220 - loss: 0.3900 - val_accuracy: 0.8149 - val_loss: 0.3888\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8328 - loss: 0.3756 - val_accuracy: 0.8436 - val_loss: 0.3409\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8418 - loss: 0.3526 - val_accuracy: 0.8393 - val_loss: 0.3411\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8529 - loss: 0.3285 - val_accuracy: 0.8587 - val_loss: 0.3060\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8622 - loss: 0.3099 - val_accuracy: 0.8402 - val_loss: 0.3376\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8699 - loss: 0.2952 - val_accuracy: 0.8893 - val_loss: 0.2588\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8792 - loss: 0.2782 - val_accuracy: 0.9007 - val_loss: 0.2383\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8860 - loss: 0.2634 - val_accuracy: 0.8802 - val_loss: 0.2635\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8860 - loss: 0.2618 - val_accuracy: 0.8904 - val_loss: 0.2488\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8972 - loss: 0.2449 - val_accuracy: 0.8831 - val_loss: 0.2588\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8933 - loss: 0.2517 - val_accuracy: 0.8784 - val_loss: 0.2639\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8958 - loss: 0.2474 - val_accuracy: 0.8382 - val_loss: 0.3506\nutils.plot_model(model_title, \"model1.png\", \n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\noutput\n\n\nplt.plot(history_title.history[\"accuracy\"], label = \"training\")\nplt.plot(history_title.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 85% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n\n\n\nWe follow the same procedure as the first model to determine whether an article contains fake news or not solely based on its text.\n# Model 2: Only article text as input\n\ntext_features = title_vectorize_layer(text_input) \ntext_features = embedding_layer(text_features) \ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\ntext_output = layers.Dense(2,name='fake')(text_features)\nmodel_text = keras.Model(\n    inputs = text_input,\n    outputs = text_output\n)\nmodel_text.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory_text = model_text.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.7124 - loss: 0.5868 - val_accuracy: 0.9056 - val_loss: 0.3190\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8891 - loss: 0.3044 - val_accuracy: 0.9287 - val_loss: 0.2259\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9191 - loss: 0.2281 - val_accuracy: 0.9391 - val_loss: 0.1856\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 11ms/step - accuracy: 0.9326 - loss: 0.1908 - val_accuracy: 0.9440 - val_loss: 0.1644\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9396 - loss: 0.1680 - val_accuracy: 0.9456 - val_loss: 0.1532\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9465 - loss: 0.1531 - val_accuracy: 0.9458 - val_loss: 0.1464\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9506 - loss: 0.1427 - val_accuracy: 0.9427 - val_loss: 0.1431\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9516 - loss: 0.1342 - val_accuracy: 0.9540 - val_loss: 0.1256\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9546 - loss: 0.1268 - val_accuracy: 0.9593 - val_loss: 0.1181\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9588 - loss: 0.1171 - val_accuracy: 0.9571 - val_loss: 0.1163\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9583 - loss: 0.1154 - val_accuracy: 0.9596 - val_loss: 0.1101\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9635 - loss: 0.1044 - val_accuracy: 0.9640 - val_loss: 0.1031\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9647 - loss: 0.1036 - val_accuracy: 0.9647 - val_loss: 0.1007\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9681 - loss: 0.0930 - val_accuracy: 0.9658 - val_loss: 0.0973\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9677 - loss: 0.0938 - val_accuracy: 0.9673 - val_loss: 0.0907\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 19ms/step - accuracy: 0.9712 - loss: 0.0891 - val_accuracy: 0.9658 - val_loss: 0.0976\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9704 - loss: 0.0857 - val_accuracy: 0.9684 - val_loss: 0.0897\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9712 - loss: 0.0843 - val_accuracy: 0.9709 - val_loss: 0.0855\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9713 - loss: 0.0808 - val_accuracy: 0.9707 - val_loss: 0.0845\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9749 - loss: 0.0762 - val_accuracy: 0.9724 - val_loss: 0.0798\nutils.plot_model(model_text, \"model2.png\", \n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\noutput\n\n\nplt.plot(history_text.history[\"accuracy\"], label = \"training\")\nplt.plot(history_text.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 97% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n\n\n\nIn the last model, we will use both the title and text of the article to determine whether the article contains fake news. Since we have already defined the layers for title_features and text_features, we can combine them to create our new model.\nmain = layers.concatenate([title_features, text_features], axis = 1) # combine the layers of title and text\n\nmain = layers.Dense(32, activation = 'relu')(main)\noutput = layers.Dense(2, name = \"fake\")(main)\n\nmodel = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\nmodel.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory = model.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9277 - loss: 0.2429 - val_accuracy: 0.9700 - val_loss: 0.0957\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9713 - loss: 0.0889 - val_accuracy: 0.9724 - val_loss: 0.0810\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9717 - loss: 0.0803 - val_accuracy: 0.9736 - val_loss: 0.0752\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9790 - loss: 0.0628 - val_accuracy: 0.9771 - val_loss: 0.0657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9722 - loss: 0.0795 - val_accuracy: 0.9780 - val_loss: 0.0636\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9800 - loss: 0.0581 - val_accuracy: 0.9787 - val_loss: 0.0582\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9830 - loss: 0.0485 - val_accuracy: 0.9813 - val_loss: 0.0519\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9826 - loss: 0.0509 - val_accuracy: 0.9762 - val_loss: 0.0658\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9821 - loss: 0.0501 - val_accuracy: 0.9820 - val_loss: 0.0522\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9843 - loss: 0.0454 - val_accuracy: 0.9780 - val_loss: 0.0610\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9857 - loss: 0.0412 - val_accuracy: 0.9804 - val_loss: 0.0543\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9848 - loss: 0.0442 - val_accuracy: 0.9824 - val_loss: 0.0481\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9832 - loss: 0.0472 - val_accuracy: 0.9811 - val_loss: 0.0514\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9868 - loss: 0.0380 - val_accuracy: 0.9796 - val_loss: 0.0574\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 18ms/step - accuracy: 0.9880 - loss: 0.0358 - val_accuracy: 0.9824 - val_loss: 0.0480\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9832 - loss: 0.0448 - val_accuracy: 0.9842 - val_loss: 0.0459\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9872 - loss: 0.0364 - val_accuracy: 0.9851 - val_loss: 0.0440\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9896 - loss: 0.0310 - val_accuracy: 0.9856 - val_loss: 0.0423\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9898 - loss: 0.0287 - val_accuracy: 0.9853 - val_loss: 0.0435\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9888 - loss: 0.0305 - val_accuracy: 0.9838 - val_loss: 0.0471\nutils.plot_model(model, \"model3.png\",\n                       show_shapes=True)\n\n\n\noutput\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 98-99% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n\n\n\n\nWe can test out our third model on unseen test data. The code below shows we have achieved 98.13% accuracy on the test data!\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(test_url)\ntest = make_dataset(test_data)\n\nmodel.evaluate(test)\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9813 - loss: 0.0577\n[0.05766301602125168, 0.9808009266853333]\n\n\n\nWe can use plotly to create an interactive plot to see how the words are related to each other. The words that the model associates with fake news will tend towards one side and the words that the model associates with real news will be on the other.\nweights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()         # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n On the far left we can see words such as “trumps”, “obamas”, as well as country names such as “chinas”, “koreas”. On the right there are words such as “shocking”, “reportedly”, “insane”."
  },
  {
    "objectID": "posts/hw6/index.html#acquire-training-data",
    "href": "posts/hw6/index.html#acquire-training-data",
    "title": "HW6",
    "section": "",
    "text": "The data that we will use to train the model can be found at the link below. We will read the data into a pandas dataframe.\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\ntrain_data = pd.read_csv(train_url)\ntrain_data\n We see that this dataset contains information about 22449 articles. Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above."
  },
  {
    "objectID": "posts/hw6/index.html#create-a-dataset",
    "href": "posts/hw6/index.html#create-a-dataset",
    "title": "HW6",
    "section": "",
    "text": "Next, let’s write the function make_dataset to convert the data to a tensorflow dataset.\nbatch_size = 100\ndef make_dataset(df):\n    # Load stopwords from NLTK\n    stop_words = stopwords.words('english')\n    \n    # remove stopwords\n    df['title'] = df['title'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    df['text'] = df['text'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    \n    # Construct a tf.data.Dataset\n    # Note: Ensure your DataFrame has columns named 'title', 'text', and 'fake' for this to work directly\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                \"title\": df['title'].values,  # Pass series.values to get numpy representation\n                \"text\": df['text'].values\n            },\n            df['fake'].values\n        )\n    )\n    \n    # Batch the dataset\n    dataset = dataset.batch(batch_size)\n    \n    return dataset\n\ndata = make_dataset(train_data)\nNow let’s split our data into training and validation sets. We will use 20% for validation. Again, we will batch the data so it will train more efficiently.\ndata = data.shuffle(buffer_size = len(data), reshuffle_each_iteration=False)\ntrain_size = int(0.8 * len(data)) # allocate 80% of data for training\n\ntrain = data.take(train_size) # set training data\nval = data.skip(train_size) # set validation data\n# check the size of training, validation, testing set\nlen(train), len(val)\n(180, 45)\nHere we see that the training data is four times larger than the validation data, which is what we expect.\nclass_counts = train_data['fake'].value_counts(normalize=True)\n\n# The base rate will be the proportion of the most common class\nbase_rate = class_counts.max()\nbase_rate\n0.522963160942581"
  },
  {
    "objectID": "posts/hw6/index.html#text-vectorization",
    "href": "posts/hw6/index.html#text-vectorization",
    "title": "HW6",
    "section": "",
    "text": "In the code below, we standardize the the data by turning all text to lowercase and removing punctuation. We also vectorize the text through creating a text vectorization layer.\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n# Define the shared embedding layer\nembedding_layer = layers.Embedding(size_vocabulary, 20, name = \"embedding\")\n\n# Define the input layers\ntitle_input = keras.Input(\n    shape = (1,), \n    # name for us to remember for later\n    name = \"title\",\n    # type of data contained\n    dtype = \"string\"\n)\n\ntext_input = keras.Input(\n    shape = (1,), \n    name = \"text\",\n    dtype = \"string\"\n)"
  },
  {
    "objectID": "posts/hw6/index.html#create-models",
    "href": "posts/hw6/index.html#create-models",
    "title": "HW6",
    "section": "",
    "text": "The first model will determine whether an article contains fake news or not solely based on its title. To do so, we will use the functional API of TensorFlow. We will define a pipeline of hidden layers to process the titles. First, we explicitly define an embedding layer. This is so we can reuse it in a later model. Then we define different layers for the text data. The dropout layers prevent overfitting.\n# Model 1: Only article title as input\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = embedding_layer(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntitle_output = layers.Dense(2,name='fake')(title_features)\nmodel_title = keras.Model(\n    inputs = title_input,\n    outputs = title_output\n)\nmodel_title.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory_title = model_title.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - accuracy: 0.5163 - loss: 0.6932 - val_accuracy: 0.5289 - val_loss: 0.6891\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5299 - loss: 0.6899 - val_accuracy: 0.5289 - val_loss: 0.6836\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5696 - loss: 0.6799 - val_accuracy: 0.7111 - val_loss: 0.6536\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6596 - loss: 0.6401 - val_accuracy: 0.7742 - val_loss: 0.5562\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7271 - loss: 0.5563 - val_accuracy: 0.7924 - val_loss: 0.4796\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7548 - loss: 0.4996 - val_accuracy: 0.8087 - val_loss: 0.4375\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7870 - loss: 0.4572 - val_accuracy: 0.8176 - val_loss: 0.4127\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8050 - loss: 0.4236 - val_accuracy: 0.8273 - val_loss: 0.3870\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8220 - loss: 0.3900 - val_accuracy: 0.8149 - val_loss: 0.3888\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8328 - loss: 0.3756 - val_accuracy: 0.8436 - val_loss: 0.3409\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8418 - loss: 0.3526 - val_accuracy: 0.8393 - val_loss: 0.3411\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8529 - loss: 0.3285 - val_accuracy: 0.8587 - val_loss: 0.3060\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8622 - loss: 0.3099 - val_accuracy: 0.8402 - val_loss: 0.3376\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8699 - loss: 0.2952 - val_accuracy: 0.8893 - val_loss: 0.2588\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8792 - loss: 0.2782 - val_accuracy: 0.9007 - val_loss: 0.2383\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8860 - loss: 0.2634 - val_accuracy: 0.8802 - val_loss: 0.2635\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8860 - loss: 0.2618 - val_accuracy: 0.8904 - val_loss: 0.2488\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8972 - loss: 0.2449 - val_accuracy: 0.8831 - val_loss: 0.2588\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8933 - loss: 0.2517 - val_accuracy: 0.8784 - val_loss: 0.2639\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8958 - loss: 0.2474 - val_accuracy: 0.8382 - val_loss: 0.3506\nutils.plot_model(model_title, \"model1.png\", \n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\noutput\n\n\nplt.plot(history_title.history[\"accuracy\"], label = \"training\")\nplt.plot(history_title.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 85% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n\n\n\nWe follow the same procedure as the first model to determine whether an article contains fake news or not solely based on its text.\n# Model 2: Only article text as input\n\ntext_features = title_vectorize_layer(text_input) \ntext_features = embedding_layer(text_features) \ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\ntext_output = layers.Dense(2,name='fake')(text_features)\nmodel_text = keras.Model(\n    inputs = text_input,\n    outputs = text_output\n)\nmodel_text.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory_text = model_text.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.7124 - loss: 0.5868 - val_accuracy: 0.9056 - val_loss: 0.3190\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8891 - loss: 0.3044 - val_accuracy: 0.9287 - val_loss: 0.2259\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9191 - loss: 0.2281 - val_accuracy: 0.9391 - val_loss: 0.1856\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 11ms/step - accuracy: 0.9326 - loss: 0.1908 - val_accuracy: 0.9440 - val_loss: 0.1644\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9396 - loss: 0.1680 - val_accuracy: 0.9456 - val_loss: 0.1532\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9465 - loss: 0.1531 - val_accuracy: 0.9458 - val_loss: 0.1464\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9506 - loss: 0.1427 - val_accuracy: 0.9427 - val_loss: 0.1431\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9516 - loss: 0.1342 - val_accuracy: 0.9540 - val_loss: 0.1256\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9546 - loss: 0.1268 - val_accuracy: 0.9593 - val_loss: 0.1181\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9588 - loss: 0.1171 - val_accuracy: 0.9571 - val_loss: 0.1163\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9583 - loss: 0.1154 - val_accuracy: 0.9596 - val_loss: 0.1101\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9635 - loss: 0.1044 - val_accuracy: 0.9640 - val_loss: 0.1031\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9647 - loss: 0.1036 - val_accuracy: 0.9647 - val_loss: 0.1007\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9681 - loss: 0.0930 - val_accuracy: 0.9658 - val_loss: 0.0973\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9677 - loss: 0.0938 - val_accuracy: 0.9673 - val_loss: 0.0907\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 19ms/step - accuracy: 0.9712 - loss: 0.0891 - val_accuracy: 0.9658 - val_loss: 0.0976\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9704 - loss: 0.0857 - val_accuracy: 0.9684 - val_loss: 0.0897\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9712 - loss: 0.0843 - val_accuracy: 0.9709 - val_loss: 0.0855\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9713 - loss: 0.0808 - val_accuracy: 0.9707 - val_loss: 0.0845\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9749 - loss: 0.0762 - val_accuracy: 0.9724 - val_loss: 0.0798\nutils.plot_model(model_text, \"model2.png\", \n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\noutput\n\n\nplt.plot(history_text.history[\"accuracy\"], label = \"training\")\nplt.plot(history_text.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 97% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model.\n\n\n\nIn the last model, we will use both the title and text of the article to determine whether the article contains fake news. Since we have already defined the layers for title_features and text_features, we can combine them to create our new model.\nmain = layers.concatenate([title_features, text_features], axis = 1) # combine the layers of title and text\n\nmain = layers.Dense(32, activation = 'relu')(main)\noutput = layers.Dense(2, name = \"fake\")(main)\n\nmodel = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\nmodel.compile(optimizer = \"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy']\n)\nhistory = model.fit(train,\n                    validation_data=val,\n                    epochs = 20)\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9277 - loss: 0.2429 - val_accuracy: 0.9700 - val_loss: 0.0957\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9713 - loss: 0.0889 - val_accuracy: 0.9724 - val_loss: 0.0810\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9717 - loss: 0.0803 - val_accuracy: 0.9736 - val_loss: 0.0752\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9790 - loss: 0.0628 - val_accuracy: 0.9771 - val_loss: 0.0657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9722 - loss: 0.0795 - val_accuracy: 0.9780 - val_loss: 0.0636\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9800 - loss: 0.0581 - val_accuracy: 0.9787 - val_loss: 0.0582\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9830 - loss: 0.0485 - val_accuracy: 0.9813 - val_loss: 0.0519\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9826 - loss: 0.0509 - val_accuracy: 0.9762 - val_loss: 0.0658\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9821 - loss: 0.0501 - val_accuracy: 0.9820 - val_loss: 0.0522\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9843 - loss: 0.0454 - val_accuracy: 0.9780 - val_loss: 0.0610\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9857 - loss: 0.0412 - val_accuracy: 0.9804 - val_loss: 0.0543\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9848 - loss: 0.0442 - val_accuracy: 0.9824 - val_loss: 0.0481\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9832 - loss: 0.0472 - val_accuracy: 0.9811 - val_loss: 0.0514\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9868 - loss: 0.0380 - val_accuracy: 0.9796 - val_loss: 0.0574\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 18ms/step - accuracy: 0.9880 - loss: 0.0358 - val_accuracy: 0.9824 - val_loss: 0.0480\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9832 - loss: 0.0448 - val_accuracy: 0.9842 - val_loss: 0.0459\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9872 - loss: 0.0364 - val_accuracy: 0.9851 - val_loss: 0.0440\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9896 - loss: 0.0310 - val_accuracy: 0.9856 - val_loss: 0.0423\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9898 - loss: 0.0287 - val_accuracy: 0.9853 - val_loss: 0.0435\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9888 - loss: 0.0305 - val_accuracy: 0.9838 - val_loss: 0.0471\nutils.plot_model(model, \"model3.png\",\n                       show_shapes=True)\n\n\n\noutput\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n We can see that the model achieves around 98-99% validation accuracy. The training and validation data appear to have similar accuracy, which indicates we did not overfit the model."
  },
  {
    "objectID": "posts/hw6/index.html#model-evaluation",
    "href": "posts/hw6/index.html#model-evaluation",
    "title": "HW6",
    "section": "",
    "text": "We can test out our third model on unseen test data. The code below shows we have achieved 98.13% accuracy on the test data!\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(test_url)\ntest = make_dataset(test_data)\n\nmodel.evaluate(test)\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9813 - loss: 0.0577\n[0.05766301602125168, 0.9808009266853333]"
  },
  {
    "objectID": "posts/hw6/index.html#embedding-visualization",
    "href": "posts/hw6/index.html#embedding-visualization",
    "title": "HW6",
    "section": "",
    "text": "We can use plotly to create an interactive plot to see how the words are related to each other. The words that the model associates with fake news will tend towards one side and the words that the model associates with real news will be on the other.\nweights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()         # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n On the far left we can see words such as “trumps”, “obamas”, as well as country names such as “chinas”, “koreas”. On the right there are words such as “shocking”, “reportedly”, “insane”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "HW6\n\n\n\n\n\n\nweek 10\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nHW4\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nHW3\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nHW5\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nHW1\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nCreating posts\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nJessica Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]